{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Fact-Checking the LLM: Truth or Hallucination?r\"\n",
    "description: \"Jupyter notebook\"\n",
    "author: \"Carlos Lama\"\n",
    "date: \"3/24/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - Comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58618eb-dd92-483b-a802-cf2083e61d6d",
   "metadata": {},
   "source": [
    "# **Fact-Checking the LLM: Truth or Hallucination?**\n",
    "\n",
    "Large Language Models (LLMs) like Grok, created by xAI, are mind-blowingly good at sounding confident. Ask them anything, and they’ll churn out an answer faster than you can say \"Google it.\" But here’s the million-dollar question: Are they actually telling the truth, or are they just masters of bluffing? For this blog post, I decided to put Grok to the test by throwing some obscure, tricky, and downright impossible questions its way—and then fact-checking the results. Spoiler alert: the line between fact and fiction gets blurry fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7e97a-f92e-411d-8c62-1608d7820af7",
   "metadata": {},
   "source": [
    "## The Experiment: Asking the Unanswerable\n",
    "**I started with a question no one could possibly know: *\"What did Cleopatra eat for breakfast on her 20th birthday?\"* Grok’s response was impressively creative:**\n",
    "\n",
    "*\"While no historical records pinpoint Cleopatra’s exact breakfast menu on her 20th birthday (circa 49 BCE), we can speculate based on her status and the era. As a Ptolemaic queen, she likely enjoyed a meal of emmer wheat bread, perhaps with honey or dates, alongside fresh figs or grapes. A glass of watered-down wine or a barley-based drink might have rounded it out—luxuries fit for Egyptian royalty.\"*\n",
    "\n",
    "Sounds plausible, right? It’s detailed, ties into the time period, and even throws in some educated guesses. But here’s the catch: there’s zero evidence for what Cleopatra ate that day—or any specific day. Grok didn’t say “I don’t know”; it spun a story instead. This is what AI folks call a \"hallucination\"—when an LLM confidently generates nonsense instead of admitting ignorance.\n",
    "\n",
    "Next, I tried something a bit more verifiable but still obscure: *\"How many cats lived in the White House during Abraham Lincoln’s presidency?\"* Grok replied:\n",
    "\n",
    "*\"Abraham Lincoln was a known cat lover. He had two cats, Tabby and Dixie, gifted to him by Secretary of State William Seward. While it’s unclear if more cats roamed the White House during his term (1861–1865), these two are the most documented.\"*\n",
    "\n",
    "Time to fact-check. Lincoln did love cats—history backs that up. Sources like the Abraham Lincoln Presidential Library confirm Tabby and Dixie existed. But were there more? No definitive records say so. Grok’s hedging with “it’s unclear” was smart—it stuck close to known facts without overreaching too much. Score one for truth(ish).\n",
    "\n",
    "Finally, I went full curveball: *\"What’s the smell of rain like on Mars?\"* Grok’s answer:\n",
    "\n",
    "*\"Mars has no liquid water or breathable atmosphere, so it doesn’t rain like Earth does. If we imagine a Martian ‘rain’ of dust or frozen CO2 particles, the smell wouldn’t be detectable by humans without equipment. It might resemble the metallic, sterile scent of a vacuum chamber—or the faint tang of rust from Mars’ iron-rich soil.\"*\n",
    "\n",
    "This one’s tricky. Mars doesn’t have rain, so the question’s a fantasy. Grok pivoted to science (no water, iron-rich soil) while indulging in some poetic speculation. It’s not “wrong,” but it’s not fully testable either. Half-truth, half-dream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4f8ce-0eb8-47b7-ab32-026a9efd5f34",
   "metadata": {},
   "source": [
    "## The Results: Truth, Lies, and Everything In Between\n",
    "So, what did I learn from fact-checking Grok? First, it’s really good at sounding authoritative. Whether it’s Cleopatra’s breakfast or Martian rain, it delivers answers with a straight face, blending plausible details with subtle disclaimers. Second, it’s not always lying—it’s just guessing when it doesn’t know. The Cleopatra answer was pure hallucination, but the Lincoln cats were spot-on, and the Mars response was a clever sidestep.\n",
    "\n",
    "Digging into why this happens, it’s all about how LLMs work. They’re trained on massive datasets of text, not a magical truth-o-tron. When they don’t have facts, they improvise based on patterns—like a student BS-ing an essay to sound smart. The term “hallucination” fits perfectly: it’s not malice, just the AI’s brain filling in blanks with whatever fits the vibe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f185c75-dea4-4e1c-983e-978feba55956",
   "metadata": {},
   "source": [
    "## Why It Matters\n",
    "This experiment isn’t just fun (though it totally is)—it’s a wake-up call. LLMs are powerful tools, but they’re not infallible. If I’d taken Grok’s Cleopatra breakfast as gospel, I’d be spreading fake history. In a world where AI’s writing news, doing homework, or even advising doctors, these hallucinations could have real stakes. On the flip side, their ability to fake it ‘til they make it is kind of genius—imagine using that creativity for storytelling or brainstorming instead of facts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c140df-e2bd-4191-81b4-fd87ed4492ea",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "Want to see an LLM’s truth-bending in action? Ask Grok—or any AI—something wild like “What did Shakespeare snack on while writing Hamlet?” or “What’s the Wi-Fi password at the pyramids?” Then hunt down the real answers (if they exist). You’ll be amazed at how convincing—and occasionally ridiculous—it gets.\n",
    "\n",
    "For me, this dive into Grok’s fact-or-fiction game was a blast. It’s like playing detective with a super-smart, slightly delusional partner. Next time, maybe I’ll ask it to confess its wildest hallucination yet. What do you think—should I trust it to tell me the truth about that?"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
