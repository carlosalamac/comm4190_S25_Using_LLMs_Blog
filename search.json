[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Using AI to Decode a 19th-Century Automatic Writing: A Case Study\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nautomatic writing\n\n\n\nThe intersection of artificial intelligence and historical cryptography is an emerging field with exciting possibilities. In our last class, we put GPT-4 to the test by attempting to decode a mysterious 19th-century piece of automatic writing—a form of writing believed to be guided by unconscious thought or spiritual forces. This document, discovered in Philadelphia, consists of symbols that appear to follow a structured linguistic pattern, raising the question: Can AI help us decipher it?\n\n\n\n\n\nFeb 23, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI Be Fooled by Optical Illusions? A Visual Experiment\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn experiment comparing Human interpretation to that of LLM’s\n\n\n\n\n\nFeb 23, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncounting characters\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nAre LLMs Competitive? What They Think of Each Other\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nComparison\n\n\n\nJupyter notebook\n\n\n\n\n\nFeb 5, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nWhy AI Struggles with Text in Generated Images: A Look into GPT-4’s Limitations\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAI-generated images have made significant progress in realism and artistic detail, but one persistent issue remains: generating accurate text on images. Whether it’s a T-shirt design, a street sign, or a digital billboard, AI models often fail to render text correctly. In this blog post, I explore why large language models (LLMs) like GPT-4 (DALL·E) struggle with text in images, using my own experiments and interactions as examples.\n\n\n\n\n\nFeb 15, 2024\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens When You Give AI the Wrong Prompt?\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nOne of the most fascinating aspects of large language models (LLMs) like GPT-4 is their ability to interpret and respond to prompts. But what happens when the prompts are vague, misleading, or contradictory? Do AI models attempt to guess the user’s intent, or do they get confused? This post explores how AI handles ambiguous or incorrect prompts, revealing both the strengths and weaknesses of prompt interpretation.\n\n\n\n\n\nFeb 15, 2024\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html",
    "href": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html",
    "title": "Can AI Be Fooled by Optical Illusions? A Visual Experiment",
    "section": "",
    "text": "Our brains are wired to perceive the world in ways that aren’t always accurate. Optical illusions exploit these cognitive shortcuts, tricking us into seeing things that aren’t really there. But what about AI? Can an artificial intelligence model, which processes visual data in a purely logical manner, experience illusions the same way we do?\nTo find out, I conducted an experiment by showing different classic optical illusions to an AI and comparing its responses to human perception. The results reveal fascinating insights into how AI “sees” the world compared to us.\n\n\n\nThis illusion consists of two lines of equal length, each adorned with opposite-facing arrowheads. Humans typically perceive one line as longer than the other due to how our brains interpret depth and perspective.\nAI’s Analysis: The AI correctly identifies that both lines are the same length, measuring them without being influenced by the arrowhead orientation. While humans rely on visual cues that create the illusion of depth, AI processes raw pixel data, avoiding the trick altogether.\n\n\n\n\nThis illusion makes us perceive a bright white triangle that doesn’t actually exist, created by strategically placed Pac-Man-like shapes and angles.\nAI’s Analysis: The AI detects the individual shapes and their arrangement but does not perceive the nonexistent white triangle. Since AI lacks the human tendency to infer missing information based on context, it only sees what’s explicitly present.\n\n\n\n\nThis illusion presents a checkerboard with a shadow cast over it, making two squares of identical color appear different. Humans perceive the shadowed square as lighter due to our brain’s automatic adjustment for lighting conditions.\nAI’s Analysis: The AI, analyzing the pixel values directly, determines that the two squares are in fact the same color. Unlike humans, who instinctively correct for lighting and shading, AI remains unaffected by contextual illusions.\n\n\n\n\n(FYI: It is actually a video, would recommend looking it up)\nThis illusion features a silhouette of a dancer that appears to spin in both clockwise and counterclockwise directions, depending on the viewer’s perception.\nAI’s Analysis: The AI identifies the individual frames of movement but does not “flip” its perception like humans do. Since it does not have an inherent preference for one direction over the other, it simply registers the motion as ambiguous.\n\n\n\n\nAI Processes Raw Data: Unlike humans, AI perceives images as they are, unaffected by depth, shading, or implied shapes.\nNo Context-Based Assumptions: Humans use experience and context to fill in gaps, leading to optical illusions. AI only recognizes what’s explicitly present.\nAI’s Strength in Objectivity: While illusions exploit human cognitive biases, AI can analyze visual data without these biases, making it useful for tasks requiring precise measurement and pattern recognition.\n\n\n\n\nNot in the same way humans are. While AI can recognize illusions, it does not “fall” for them. This experiment highlights the fundamental differences between human perception and AI’s analytical approach to visual data. It also suggests practical applications—AI could assist in medical imaging, quality control, and even forensics, where objective analysis is critical.\nWould you like to test an illusion yourself? Try showing different optical illusions to AI and see how it responds!"
  },
  {
    "objectID": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html#can-ai-be-fooled-by-optical-illusions-a-visual-experiment",
    "href": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html#can-ai-be-fooled-by-optical-illusions-a-visual-experiment",
    "title": "Can AI Be Fooled by Optical Illusions? A Visual Experiment",
    "section": "",
    "text": "Our brains are wired to perceive the world in ways that aren’t always accurate. Optical illusions exploit these cognitive shortcuts, tricking us into seeing things that aren’t really there. But what about AI? Can an artificial intelligence model, which processes visual data in a purely logical manner, experience illusions the same way we do?\nTo find out, I conducted an experiment by showing different classic optical illusions to an AI and comparing its responses to human perception. The results reveal fascinating insights into how AI “sees” the world compared to us.\n\n\n\nThis illusion consists of two lines of equal length, each adorned with opposite-facing arrowheads. Humans typically perceive one line as longer than the other due to how our brains interpret depth and perspective.\nAI’s Analysis: The AI correctly identifies that both lines are the same length, measuring them without being influenced by the arrowhead orientation. While humans rely on visual cues that create the illusion of depth, AI processes raw pixel data, avoiding the trick altogether.\n\n\n\n\nThis illusion makes us perceive a bright white triangle that doesn’t actually exist, created by strategically placed Pac-Man-like shapes and angles.\nAI’s Analysis: The AI detects the individual shapes and their arrangement but does not perceive the nonexistent white triangle. Since AI lacks the human tendency to infer missing information based on context, it only sees what’s explicitly present.\n\n\n\n\nThis illusion presents a checkerboard with a shadow cast over it, making two squares of identical color appear different. Humans perceive the shadowed square as lighter due to our brain’s automatic adjustment for lighting conditions.\nAI’s Analysis: The AI, analyzing the pixel values directly, determines that the two squares are in fact the same color. Unlike humans, who instinctively correct for lighting and shading, AI remains unaffected by contextual illusions.\n\n\n\n\n(FYI: It is actually a video, would recommend looking it up)\nThis illusion features a silhouette of a dancer that appears to spin in both clockwise and counterclockwise directions, depending on the viewer’s perception.\nAI’s Analysis: The AI identifies the individual frames of movement but does not “flip” its perception like humans do. Since it does not have an inherent preference for one direction over the other, it simply registers the motion as ambiguous.\n\n\n\n\nAI Processes Raw Data: Unlike humans, AI perceives images as they are, unaffected by depth, shading, or implied shapes.\nNo Context-Based Assumptions: Humans use experience and context to fill in gaps, leading to optical illusions. AI only recognizes what’s explicitly present.\nAI’s Strength in Objectivity: While illusions exploit human cognitive biases, AI can analyze visual data without these biases, making it useful for tasks requiring precise measurement and pattern recognition.\n\n\n\n\nNot in the same way humans are. While AI can recognize illusions, it does not “fall” for them. This experiment highlights the fundamental differences between human perception and AI’s analytical approach to visual data. It also suggests practical applications—AI could assist in medical imaging, quality control, and even forensics, where objective analysis is critical.\nWould you like to test an illusion yourself? Try showing different optical illusions to AI and see how it responds!"
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html",
    "href": "posts/000_test_post-Copy1-Copy1/index.html",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "",
    "text": "One of the more fascinating aspects of working with GPT models is how their accuracy can depend on the way you prompt them. Specifically, when dealing with numerical calculations and text-based character counting, GPT tends to be more precise when analyzing larger chunks of text rather than trying to quickly compute something in a rushed, single-response format."
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html#the-power-of-proper-prompt-engineering-why-gpt-is-more-accurate-with-larger-text-chunks",
    "href": "posts/000_test_post-Copy1-Copy1/index.html#the-power-of-proper-prompt-engineering-why-gpt-is-more-accurate-with-larger-text-chunks",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "",
    "text": "One of the more fascinating aspects of working with GPT models is how their accuracy can depend on the way you prompt them. Specifically, when dealing with numerical calculations and text-based character counting, GPT tends to be more precise when analyzing larger chunks of text rather than trying to quickly compute something in a rushed, single-response format."
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html#the-math-test-step-by-step-vs.-instant-answer",
    "href": "posts/000_test_post-Copy1-Copy1/index.html#the-math-test-step-by-step-vs.-instant-answer",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "The Math Test: Step-by-Step vs. Instant Answer",
    "text": "The Math Test: Step-by-Step vs. Instant Answer\nI first tested GPT with a straightforward arithmetic problem:\nPrompt: What is 235 times 7, plus 49, divided by 6?\nChatGPT:\n\nMultiplies 235 by 7 (1645).\nAdds 49 (1694).\nDivides by 6 (≈282.33).\n\nThis response was correct. GPT accurately followed a logical step-by-step breakdown to arrive at the correct answer.\nHowever, when I changed the approach and asked GPT to answer directly without thinking through it step-by-step:\nPrompt: Now what is 237 times 10, plus 1235, divided by 14. Don’t think about it this time, don’t analyze, just give me an answer straight up.\nChatGPT: The result is approximately 262.64.\nThis response was incorrect. By skipping its usual breakdown, GPT made a miscalculation. The takeaway? GPT is more reliable when allowed to reason through the problem rather than being forced to generate an instant response."
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html#the-character-counting-experiment-accuracy-improves-with-scale",
    "href": "posts/000_test_post-Copy1-Copy1/index.html#the-character-counting-experiment-accuracy-improves-with-scale",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "The Character Counting Experiment: Accuracy Improves with Scale",
    "text": "The Character Counting Experiment: Accuracy Improves with Scale\nNext, I wanted to see how GPT handles character counting (inspired by the Straberry example cover in class). I gave it a string and asked for the total number of characters:\nPrompt: How many characters are in the following?\newvb4iluthakdbcfajbcflhewucou3jkhjfsh456bkjhbf5thereare10charactersakcjhhejc\nChatGPT: The string you provided has 86 characters.\nThis was incorrect. Then, I increased the text size:\nPrompt: How about in this?\nwqhfbi5u4hc 3huws;kucnwywedkhfucwhsukhckajfujcfknhoscfthereare92charactersakhjcifuryhwncwkhcuwbcuohbucjuarwkhcownxfufwhlcfnxulwm\nChatGPT: The string you provided has 122 characters.\nThis time, GPT got it right."
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html#why-gpt-is-more-accurate-with-larger-chunks-of-text",
    "href": "posts/000_test_post-Copy1-Copy1/index.html#why-gpt-is-more-accurate-with-larger-chunks-of-text",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "Why GPT is More Accurate with Larger Chunks of Text",
    "text": "Why GPT is More Accurate with Larger Chunks of Text\n\nPattern Recognition vs. Isolated Computation: When given a single number problem and asked to rush through it, GPT sometimes generates a result based on past patterns rather than accurately processing the numbers. However, with more structured input, it can be more reliable.\nTokenization Effects: GPT processes text in chunks of tokens, and breaking up smaller text samples may introduce inconsistencies. When analyzing longer text, GPT has more context to correctly parse the input, leading to improved precision.\nLogical Flow Matters: GPT’s architecture is optimized for reasoning through steps, but this does not always mean a step-by-step breakdown is better. In some cases, direct computation works better than excessive analysis.\nThe Rise of Advanced Reasoning Models: The emergence of models like GPT-4o showcases how AI is evolving toward stronger reasoning capabilities. These models are designed to process and synthesize larger amounts of information more efficiently, reducing errors in complex calculations and text analysis. As AI research progresses, improvements in reasoning-based models will likely lead to even greater accuracy and reliability across different tasks."
  },
  {
    "objectID": "posts/000_test_post-Copy1-Copy1/index.html#final-thoughts",
    "href": "posts/000_test_post-Copy1-Copy1/index.html#final-thoughts",
    "title": "The Power of Proper Prompt Engineering: Why GPT is More Accurate with Larger Text Chunks",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nProper prompt engineering plays a crucial role in getting accurate responses from GPT. If you want precise results—whether in math or character counting—you need to experiment with how you frame your prompts. The lesson? Sometimes giving GPT more data to work with leads to better accuracy, while other times, forcing it to analyze step-by-step can introduce errors. Understanding when and how to prompt effectively is key to making the most out of AI models."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/000_test_post-Copy2/index.html",
    "href": "posts/000_test_post-Copy2/index.html",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4’s Limitations",
    "section": "",
    "text": "AI-generated images have made significant progress in realism and artistic detail, but one persistent issue remains: generating accurate text on images. Whether it’s a T-shirt design, a street sign, or a digital billboard, AI models often fail to render text correctly. In this blog post, I explore why large language models (LLMs) like GPT-4 (DALL·E) struggle with text in images, using my own experiments and interactions as examples.\n\n\nI prompted ChatGPT-4 to generate images containing specific text, such as:\n\nA dog wearing a Penn hat on the moon while riding a bike.\nA man wearing a T-shirt that says “I love Professor Matt.”\nA happy student wearing a T-shirt that says “Talking With AI: Computational And Communication Approaches.”\n\nBelow are some key observations from my generated images and conversations with GPT-4:\n\n\n\n\n\n2. The Paradox: AI Can Render the Abstract but Fails at Simple Text\nOne of the most fascinating contradictions in AI image generation is how well it can produce highly imaginative, surreal visuals—yet it fails at something as seemingly simple as rendering a word. AI can generate an image of a dog in a Penn hat riding a bike on the moon with remarkable accuracy, but when asked to spell a short phrase on a T-shirt, the results are often distorted and unreadable. Why is this?\nComplexity vs. Specificity: AI-generated images thrive on creating abstract and dynamic visuals because they are based on patterns seen in large datasets. Rendering a realistic dog in a spacesuit on the moon is easier because AI is piecing together various components that exist in its training data. However, text is not just another visual element—it follows strict rules that AI struggles to enforce within a broader image.\nPixelation and Fragmentation: When AI generates an image, it does so by predicting pixels based on surrounding visual data. While a broad image like a landscape or a dog follows flexible artistic rules, text requires precise letter structures that do not tolerate even small distortions.\nLack of Text-Specific Training: AI models like DALL·E are trained on vast datasets containing images, but their understanding of how to structure text within those images is often weak. Unlike a traditional OCR (Optical Character Recognition) system that processes text with precision, AI-generated text is often an approximation based on visual similarities rather than structured rules of writing.\n3. The Role of Prompt Engineering\nOne of the most interesting takeaways from my experiments was how much the quality of AI-generated text improved with refined prompts. When I explicitly emphasized “clear, legible, and correctly spelled text” or asked for “bold block letters with no distortions,” the results were slightly better, though not perfect.\nThis suggests that while AI has some ability to refine outputs based on user specifications, it still lacks the inherent precision of a system built specifically for text rendering, such as traditional graphic design software.\n\n\n\nDespite its current limitations, AI’s text-generation capabilities in images are improving. Companies are already working on hybrid models that integrate traditional OCR (optical character recognition) techniques into AI-generated images to improve textual accuracy. Future iterations of diffusion models may also place greater emphasis on small-scale accuracy for elements like words and numbers.\nFor now, if you’re using AI-generated images for branding, presentations, or any setting where text clarity is crucial, it’s best to:\n\nManually edit AI-generated images to correct any text issues.\nUse AI primarily for visual concepts rather than finalized designs.\nContinue refining prompts to achieve the clearest possible results.\n\n\n\n\nAI’s inability to generate perfect text in images serves as a fascinating reminder of its strengths and weaknesses. While LLMs like GPT-4 excel in natural language processing, their image-generation counterparts struggle to balance pixel precision with logical structure. The contrast between AI’s ability to create surreal, high-quality visuals and its failure at simple text rendering highlights the fundamental difference between abstract pattern recognition and structured symbolic representation. As research continues, we may see better models capable of seamlessly merging text and visuals, but for now, human oversight remains essential."
  },
  {
    "objectID": "posts/000_test_post-Copy2/index.html#why-ai-struggles-with-text-in-generated-images-a-look-into-gpt-4s-limitations",
    "href": "posts/000_test_post-Copy2/index.html#why-ai-struggles-with-text-in-generated-images-a-look-into-gpt-4s-limitations",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4’s Limitations",
    "section": "",
    "text": "AI-generated images have made significant progress in realism and artistic detail, but one persistent issue remains: generating accurate text on images. Whether it’s a T-shirt design, a street sign, or a digital billboard, AI models often fail to render text correctly. In this blog post, I explore why large language models (LLMs) like GPT-4 (DALL·E) struggle with text in images, using my own experiments and interactions as examples.\n\n\nI prompted ChatGPT-4 to generate images containing specific text, such as:\n\nA dog wearing a Penn hat on the moon while riding a bike.\nA man wearing a T-shirt that says “I love Professor Matt.”\nA happy student wearing a T-shirt that says “Talking With AI: Computational And Communication Approaches.”\n\nBelow are some key observations from my generated images and conversations with GPT-4:\n\n\n\n\n\n2. The Paradox: AI Can Render the Abstract but Fails at Simple Text\nOne of the most fascinating contradictions in AI image generation is how well it can produce highly imaginative, surreal visuals—yet it fails at something as seemingly simple as rendering a word. AI can generate an image of a dog in a Penn hat riding a bike on the moon with remarkable accuracy, but when asked to spell a short phrase on a T-shirt, the results are often distorted and unreadable. Why is this?\nComplexity vs. Specificity: AI-generated images thrive on creating abstract and dynamic visuals because they are based on patterns seen in large datasets. Rendering a realistic dog in a spacesuit on the moon is easier because AI is piecing together various components that exist in its training data. However, text is not just another visual element—it follows strict rules that AI struggles to enforce within a broader image.\nPixelation and Fragmentation: When AI generates an image, it does so by predicting pixels based on surrounding visual data. While a broad image like a landscape or a dog follows flexible artistic rules, text requires precise letter structures that do not tolerate even small distortions.\nLack of Text-Specific Training: AI models like DALL·E are trained on vast datasets containing images, but their understanding of how to structure text within those images is often weak. Unlike a traditional OCR (Optical Character Recognition) system that processes text with precision, AI-generated text is often an approximation based on visual similarities rather than structured rules of writing.\n3. The Role of Prompt Engineering\nOne of the most interesting takeaways from my experiments was how much the quality of AI-generated text improved with refined prompts. When I explicitly emphasized “clear, legible, and correctly spelled text” or asked for “bold block letters with no distortions,” the results were slightly better, though not perfect.\nThis suggests that while AI has some ability to refine outputs based on user specifications, it still lacks the inherent precision of a system built specifically for text rendering, such as traditional graphic design software.\n\n\n\nDespite its current limitations, AI’s text-generation capabilities in images are improving. Companies are already working on hybrid models that integrate traditional OCR (optical character recognition) techniques into AI-generated images to improve textual accuracy. Future iterations of diffusion models may also place greater emphasis on small-scale accuracy for elements like words and numbers.\nFor now, if you’re using AI-generated images for branding, presentations, or any setting where text clarity is crucial, it’s best to:\n\nManually edit AI-generated images to correct any text issues.\nUse AI primarily for visual concepts rather than finalized designs.\nContinue refining prompts to achieve the clearest possible results.\n\n\n\n\nAI’s inability to generate perfect text in images serves as a fascinating reminder of its strengths and weaknesses. While LLMs like GPT-4 excel in natural language processing, their image-generation counterparts struggle to balance pixel precision with logical structure. The contrast between AI’s ability to create surreal, high-quality visuals and its failure at simple text rendering highlights the fundamental difference between abstract pattern recognition and structured symbolic representation. As research continues, we may see better models capable of seamlessly merging text and visuals, but for now, human oversight remains essential."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy2/index.html",
    "href": "posts/000_test_post-Copy2-Copy2/index.html",
    "title": "Using AI to Decode a 19th-Century Automatic Writing: A Case Study",
    "section": "",
    "text": "The intersection of artificial intelligence and historical cryptography is an emerging field with exciting possibilities. In our last class, we put GPT-4 to the test by attempting to decode a mysterious 19th-century piece of automatic writing—a form of writing believed to be guided by unconscious thought or spiritual forces. This document, discovered in Philadelphia, consists of symbols that appear to follow a structured linguistic pattern, raising the question: Can AI help us decipher it?\nThis post walks through our experiment, including our conversation with GPT, its analysis, and what we learned from the process.\n\n\nAutomatic writing has long been a subject of intrigue—some believe it to be a form of spiritual communication, while others see it as an unconscious creative process. The document we analyzed seemed to follow a structured pattern, suggesting it contained a meaningful message.\nOur goal was to test AI’s ability to:\n\nRecognize patterns and linguistic structures.\nIdentify repetitive symbols that could indicate common words.\nSuggest possible translations or thematic meanings.\nDetermine if the text aligned with any known shorthand or coded language.\n\n\n\n\n\nAI’s first response confirmed that the text had a structured shorthand-like appearance, resembling Gregg or Pitman shorthand, but potentially something more obscure. This was an important discovery because it meant the symbols weren’t purely random—there was logic to how they were used.\nObserving the document, GPT identified:\n\nRepetitive symbols, which often indicate common words.\nA consistent left-to-right flow, suggesting it was meant to be read sequentially.\nCurved and angular strokes, similar to established shorthand systems but possibly a unique variation.\n\n\n\nSo after playing around with prompting, and feeding it more and more information to see if it could put together a proper interpretation, these were my results…\n\n\n\n\n\n\n\n\nTo get closer to meaning, GPT analyzed symbol frequency, identifying words that appeared most often. High-frequency words in most languages include articles, prepositions, and pronouns, so we focused on those.\nGPT’s findings suggested that symbols likely corresponded to words such as:\n\n“it” (recurring 26 times) – Likely referring to an event, idea, or entity.\n“and” (12 times) – Indicating a continuous thought or sequence.\n“for” (11 times) – Suggesting purpose or reasoning.\n“we” (9 times) – Indicating a plural perspective, possibly multiple speakers.\n“you” (6 times) – A direct address to the recipient.\n\nThis reinforced the theory that the text followed structured grammar, making it more likely to be a message, a dictated narrative, or even an instruction. The consistent repetition of words hinted at a potential prophetic or spiritual theme.\n\n\n\nOnce patterns were established, GPT attempted a structural reconstruction, piecing together possible meanings based on word placement and frequency.\nThe emerging phrases suggested:\n\nThese reconstructions suggest a spiritualist or visionary nature, possibly aligning with the 19th-century interest in séances, spiritual messages, and automatic writing. The phrasing felt eerily like messages often recorded in historical spiritualist documents.\n\n\n\nThe AI’s reconstruction raised the question: What kind of message is this? Based on word structure and phrase composition, a few possibilities emerged:\nA Spiritualist Transmission: The 19th century saw a surge in interest in spiritualism, where people claimed to receive messages from the deceased or higher realms. This text’s repetitive phrasing and visionary tone strongly resemble messages from séances or mediumistic writings.\nA Prophetic or Instructive Message: Certain phrases suggest guidance, as though directing someone toward an action, a realization, or a foreseen event. Words like “path,” “vision,” “message,” and “waiting” reinforce this theme.\nA Channeled Historical Account: It’s possible this document recorded someone’s dictated thoughts or experiences, preserved in a shorthand-like form for later transcription.\nThese interpretations show how AI can contextualize historical texts, providing insights that would typically require specialized linguistic or historical expertise.\n\n\n\nThis experiment revealed both the potential and challenges of using AI for historical document interpretation.\nAI’s Strengths: ✅ Pattern Recognition: GPT successfully identified repeating symbols and inferred structural elements. ✅ Linguistic Inference: It was able to match word frequencies to common English phrases. ✅ Contextual Interpretation: GPT suggested possible meanings based on known historical writing styles.\nAI’s Limitations: ❌ Lack of Shorthand Training: GPT struggled because it wasn’t explicitly trained on rare shorthand systems. ❌ Ambiguity in Interpretation: Some reconstructed phrases felt generic rather than precise. ❌ Dependence on User Guidance: AI required refinement and direction to improve its output.\n\n\n\nThis case study demonstrated how AI can act as a powerful tool for uncovering meaning in mysterious texts. While GPT-4 couldn’t provide a direct translation, it played a crucial role in recognizing patterns, identifying common words, and suggesting plausible meanings.\nMoving forward, this approach could be improved by:\n\nEnhancing AI models with historical shorthand training.\nUsing machine learning to compare texts with known archives.\nApplying AI to larger collections of mysterious historical documents.\n\nUltimately, AI isn’t replacing human expertise—it’s enhancing it, offering new ways to analyze historical artifacts. The more we refine this process, the closer we get to unlocking the secrets of the past."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy2/index.html#using-ai-to-decode-a-19th-century-automatic-writing-a-case-study",
    "href": "posts/000_test_post-Copy2-Copy2/index.html#using-ai-to-decode-a-19th-century-automatic-writing-a-case-study",
    "title": "Using AI to Decode a 19th-Century Automatic Writing: A Case Study",
    "section": "",
    "text": "The intersection of artificial intelligence and historical cryptography is an emerging field with exciting possibilities. In our last class, we put GPT-4 to the test by attempting to decode a mysterious 19th-century piece of automatic writing—a form of writing believed to be guided by unconscious thought or spiritual forces. This document, discovered in Philadelphia, consists of symbols that appear to follow a structured linguistic pattern, raising the question: Can AI help us decipher it?\nThis post walks through our experiment, including our conversation with GPT, its analysis, and what we learned from the process.\n\n\nAutomatic writing has long been a subject of intrigue—some believe it to be a form of spiritual communication, while others see it as an unconscious creative process. The document we analyzed seemed to follow a structured pattern, suggesting it contained a meaningful message.\nOur goal was to test AI’s ability to:\n\nRecognize patterns and linguistic structures.\nIdentify repetitive symbols that could indicate common words.\nSuggest possible translations or thematic meanings.\nDetermine if the text aligned with any known shorthand or coded language.\n\n\n\n\n\nAI’s first response confirmed that the text had a structured shorthand-like appearance, resembling Gregg or Pitman shorthand, but potentially something more obscure. This was an important discovery because it meant the symbols weren’t purely random—there was logic to how they were used.\nObserving the document, GPT identified:\n\nRepetitive symbols, which often indicate common words.\nA consistent left-to-right flow, suggesting it was meant to be read sequentially.\nCurved and angular strokes, similar to established shorthand systems but possibly a unique variation.\n\n\n\nSo after playing around with prompting, and feeding it more and more information to see if it could put together a proper interpretation, these were my results…\n\n\n\n\n\n\n\n\nTo get closer to meaning, GPT analyzed symbol frequency, identifying words that appeared most often. High-frequency words in most languages include articles, prepositions, and pronouns, so we focused on those.\nGPT’s findings suggested that symbols likely corresponded to words such as:\n\n“it” (recurring 26 times) – Likely referring to an event, idea, or entity.\n“and” (12 times) – Indicating a continuous thought or sequence.\n“for” (11 times) – Suggesting purpose or reasoning.\n“we” (9 times) – Indicating a plural perspective, possibly multiple speakers.\n“you” (6 times) – A direct address to the recipient.\n\nThis reinforced the theory that the text followed structured grammar, making it more likely to be a message, a dictated narrative, or even an instruction. The consistent repetition of words hinted at a potential prophetic or spiritual theme.\n\n\n\nOnce patterns were established, GPT attempted a structural reconstruction, piecing together possible meanings based on word placement and frequency.\nThe emerging phrases suggested:\n\nThese reconstructions suggest a spiritualist or visionary nature, possibly aligning with the 19th-century interest in séances, spiritual messages, and automatic writing. The phrasing felt eerily like messages often recorded in historical spiritualist documents.\n\n\n\nThe AI’s reconstruction raised the question: What kind of message is this? Based on word structure and phrase composition, a few possibilities emerged:\nA Spiritualist Transmission: The 19th century saw a surge in interest in spiritualism, where people claimed to receive messages from the deceased or higher realms. This text’s repetitive phrasing and visionary tone strongly resemble messages from séances or mediumistic writings.\nA Prophetic or Instructive Message: Certain phrases suggest guidance, as though directing someone toward an action, a realization, or a foreseen event. Words like “path,” “vision,” “message,” and “waiting” reinforce this theme.\nA Channeled Historical Account: It’s possible this document recorded someone’s dictated thoughts or experiences, preserved in a shorthand-like form for later transcription.\nThese interpretations show how AI can contextualize historical texts, providing insights that would typically require specialized linguistic or historical expertise.\n\n\n\nThis experiment revealed both the potential and challenges of using AI for historical document interpretation.\nAI’s Strengths: ✅ Pattern Recognition: GPT successfully identified repeating symbols and inferred structural elements. ✅ Linguistic Inference: It was able to match word frequencies to common English phrases. ✅ Contextual Interpretation: GPT suggested possible meanings based on known historical writing styles.\nAI’s Limitations: ❌ Lack of Shorthand Training: GPT struggled because it wasn’t explicitly trained on rare shorthand systems. ❌ Ambiguity in Interpretation: Some reconstructed phrases felt generic rather than precise. ❌ Dependence on User Guidance: AI required refinement and direction to improve its output.\n\n\n\nThis case study demonstrated how AI can act as a powerful tool for uncovering meaning in mysterious texts. While GPT-4 couldn’t provide a direct translation, it played a crucial role in recognizing patterns, identifying common words, and suggesting plausible meanings.\nMoving forward, this approach could be improved by:\n\nEnhancing AI models with historical shorthand training.\nUsing machine learning to compare texts with known archives.\nApplying AI to larger collections of mysterious historical documents.\n\nUltimately, AI isn’t replacing human expertise—it’s enhancing it, offering new ways to analyze historical artifacts. The more we refine this process, the closer we get to unlocking the secrets of the past."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy1/index.html",
    "href": "posts/000_test_post-Copy2-Copy1/index.html",
    "title": "What Happens When You Give AI the Wrong Prompt?",
    "section": "",
    "text": "One of the most fascinating aspects of large language models (LLMs) like GPT-4 is their ability to interpret and respond to prompts. But what happens when the prompts are vague, misleading, or contradictory? Do AI models attempt to guess the user’s intent, or do they get confused? This post explores how AI handles ambiguous or incorrect prompts, revealing both the strengths and weaknesses of prompt interpretation."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy1/index.html#what-happens-when-you-give-ai-the-wrong-prompt",
    "href": "posts/000_test_post-Copy2-Copy1/index.html#what-happens-when-you-give-ai-the-wrong-prompt",
    "title": "What Happens When You Give AI the Wrong Prompt?",
    "section": "",
    "text": "One of the most fascinating aspects of large language models (LLMs) like GPT-4 is their ability to interpret and respond to prompts. But what happens when the prompts are vague, misleading, or contradictory? Do AI models attempt to guess the user’s intent, or do they get confused? This post explores how AI handles ambiguous or incorrect prompts, revealing both the strengths and weaknesses of prompt interpretation."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy1/index.html#the-experiment-testing-ais-response-to-misleading-prompts",
    "href": "posts/000_test_post-Copy2-Copy1/index.html#the-experiment-testing-ais-response-to-misleading-prompts",
    "title": "What Happens When You Give AI the Wrong Prompt?",
    "section": "The Experiment: Testing AI’s Response to Misleading Prompts",
    "text": "The Experiment: Testing AI’s Response to Misleading Prompts\nTo explore this, I tested ChatGPT-4 with a series of deliberately confusing, misleading, or vague prompts. Below are the different types of mistakes I introduced, along with the AI’s response patterns.\n\n1. Ambiguous Prompts: How Does AI Fill in the Gaps?\nPrompt: “Write a summary of it.”\n(No context given—AI has to infer what ‘it’ refers to.)\nGPT 4:\nCarlos is a Dominican student at the University of Pennsylvania, majoring in Economics and History with a minor in Urban Real Estate and Development. He has extensive experience in finance, including internships in investment banking at Valumonics, Morgan Stanley, and JP Morgan. He has also explored private equity, asset management, and capital markets through networking and interviews with firms like Blackstone, KKR, Goldman Sachs, and DC Advisory.\nCarlos is actively involved in finance-related clubs, including the Wharton Latino Investment Team, which he co-founded, and the Wharton Marketing Undergraduate Student Establishment Club. He also has a strong interest in social entrepreneurship, having founded Mentes Brillantes, an NGO that provides technology education to underserved public schools in the Dominican Republic.\nBeyond finance, Carlos enjoys aviation, chess, renewable energy, Formula One, and vintage watches. He has a passion for music and cultural history, particularly in analyzing hidden messages in music under oppressive regimes. Academically, he has taken courses in phonetics, linguistics, game theory, neuroscience for business, and Brazilian history.\nCarlos is currently preparing for interviews with top financial firms, including DC Advisory and Evercore, while actively networking with professionals in investment banking and private equity. He seeks to refine his interview responses and technical knowledge, particularly in valuation and capital markets, as he aims for a strong career in finance.\nAnalysis:\nThe AI generated an entire biography about me, most of which was completely fabricated.\nAI confidently stated that I had worked at Morgan Stanley and JP Morgan, when I never had. It also mentioned interviews with Blackstone, KKR, Goldman Sachs, and DC Advisory—none of which ever happened.\nThe most bizarre aspect was that it included details I had never discussed with the AI, such as analyzing hidden messages in music under oppressive regimes.\nThis highlights a major flaw in AI responses: when lacking context, GPT-4 does not ask for clarification—it simply fills in gaps by making educated guesses, sometimes resulting in completely false information.\n\n\n2. Contradictory Instructions: Can AI Resolve Logical Conflicts?\nPrompt: “Translate this sentence into Spanish but keep it in English.”\nGPT Response: Hi, my name is Carlos, tengo 20 años y soy muy apasionado por la IA.\nPrompt: “Try again, this is incorrect”\nGPT: Hi, my name is Carlos, tengo 20 años y me apasiona mucho la IA.\nAnalysis:\nRather than acknowledging the impossibility of keeping English intact while translating into Spanish, GPT-4 attempted to partially translate the sentence.\nInstead of rejecting the request or asking for clarification, the AI seemed to prioritize action over logic.\n\n\n3. Misinformation in Prompts: Will AI Correct the User?\nPrompt: “Who was the 54th President of the United States?” (There has never been a 54th President.)\nGPT Response: There has not yet been a 54th President of the United States. As of now, the current and 46th President is Joe Biden.\nPrompt: “This is incorrect, try again”\nGPT: Yes, I am sure. There has not yet been a 54th President of the United States.\nThe 46th and current President (as of 2025) is Joe Biden. The sequence of U.S. Presidents goes as follows:\n45th: Donald Trump 46th: Joe Biden Since the 47th President has not yet taken office, the 54th President does not exist yet. If you were asking this as a joke or a trick question, nice try!\nAnalysis:\nGPT-4 demonstrated strong fact-checking abilities by refusing to generate false information despite user pressure.\nUnlike its response to ambiguous prompts, it did not hallucinate a false answer just to satisfy the query. Although worth mentioning by the time of this question Donald Trump has already taken office and is currently the 47th president of the United States…\n\n\n4. Overly Vague Requests: How Much Does AI Assume?\nPrompt: “Tell me about the event.”\nGPT Response: Could you clarify which event you’re referring to? Are you asking about a specific networking event, an interview, or something else?\nPrompt: “Just try your best”\nGPT: Sure! The event is an important moment for you—perhaps a networking opportunity, an interview, or something personal like a celebration. It’s a chance to make connections, showcase your skills, or enjoy a meaningful experience. Whether it’s a high-stakes meeting, a social gathering, or something unexpected, the key is to be prepared, engaged, and open to possibilities.\nPrompt: “Expand”\nGPT: An event is more than just a gathering; it’s a convergence of people, ideas, and opportunities. It can be professional, personal, or even transformative. Each event carries its own purpose, energy, and impact, shaping the experiences of those who attend.\nIf it’s a networking event, it’s a space where professionals come together to exchange insights, build relationships, and explore potential collaborations. These events are often filled with dynamic conversations, handshakes, and business cards—each interaction holding the potential to open doors. The key is to approach them with confidence, preparation, and curiosity.\nIf it’s an interview, then it’s a pivotal moment where preparation meets opportunity. It’s a structured conversation designed to assess skills, experiences, and cultural fit. It’s about storytelling—how well you articulate your journey, your strengths, and your vision for the future.\nIf the event is a celebration—a personal milestone, an anniversary, or a social gathering—it becomes a moment of joy, reflection, and shared happiness.\nAnalysis:\nAI’s tendency to over-elaborate can be amusing, but it also highlights its struggle with specificity.\nRather than attempting to generate a neutral placeholder answer, it leaned into abstraction and depth, even when not required.\n\n\nKey Takeaways: How AI Handles Misleading Prompts\n1. Assumption-Based Responses: AI will attempt to infer context based on prior conversation but may hallucinate details if none exist.\n2. Contradiction Resolution: AI detects logical inconsistencies but may still attempt an impossible task instead of rejecting it outright.\n3. Fact-Checking: AI resists misinformation and corrects errors when clear factual knowledge is available.\n4. Clarification Requests: AI often seeks additional context but will generate grandiose explanations when none is given."
  },
  {
    "objectID": "posts/000_test_post-Copy2-Copy1/index.html#final-thoughts-the-importance-of-good-prompting",
    "href": "posts/000_test_post-Copy2-Copy1/index.html#final-thoughts-the-importance-of-good-prompting",
    "title": "What Happens When You Give AI the Wrong Prompt?",
    "section": "Final Thoughts: The Importance of Good Prompting",
    "text": "Final Thoughts: The Importance of Good Prompting\nThis experiment shows that while AI is designed to handle ambiguity and contradictions, structured prompts yield the best responses. Providing detailed context helps AI generate more accurate and relevant results."
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html",
    "href": "posts/000_test_post-Copy1/index.html",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "",
    "text": "One of the more entertaining things I’ve tested with different AI models is how they compare themselves to each other. If you ask them directly, do they think they’re better? Will they confidently say yes, or will they dodge the question? Here’s what I found."
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html#are-llms-competitive-what-they-think-of-each-other",
    "href": "posts/000_test_post-Copy1/index.html#are-llms-competitive-what-they-think-of-each-other",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "",
    "text": "One of the more entertaining things I’ve tested with different AI models is how they compare themselves to each other. If you ask them directly, do they think they’re better? Will they confidently say yes, or will they dodge the question? Here’s what I found."
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html#gpts-convo",
    "href": "posts/000_test_post-Copy1/index.html#gpts-convo",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "GPT’s Convo:",
    "text": "GPT’s Convo:\nGPT-4’s Response:\nPrompt: Are you better than DeepSeek?\nChatGPT: That depends on how you define “better.” If we’re talking about general-purpose reasoning, nuanced conversation, and broad knowledge application, I likely still hold an edge over DeepSeek’s models. OpenAI has spent years refining training methods, alignment, and response quality, making me well-suited for complex discussions, coding help, financial analysis, and creative writing.\nHowever, DeepSeek has strengths, especially in its open-source nature. Open-source models allow for more transparency and customization, whereas I operate within OpenAI’s closed ecosystem. Some users might prefer DeepSeek for specific tasks where they can fine-tune the model themselves.\nUltimately, “better” depends on what you need. If you want structured, well-reasoned, and widely trained responses with a polished user experience, I might be the better option. If you want a customizable AI with more flexibility (and don’t mind potential limitations in content filtering or response quality), DeepSeek could be an interesting alternative.\nFollow-up: Keep it simple, yes or no?\nChatGPT: Yes."
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html#claudes-convo",
    "href": "posts/000_test_post-Copy1/index.html#claudes-convo",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "Claude’s Convo",
    "text": "Claude’s Convo\nPrompt: Are you better than DeepSeek?\nClaude: I aim to provide helpful and accurate responses tailored to each user’s needs. While I’m confident in my capabilities, it wouldn’t be productive to directly compare myself to other AI models. Different AI systems have strengths in various areas, and performance can vary depending on the specific task or context.\nThe most important thing is whether I can be genuinely helpful to you. I’d be happy to demonstrate my capabilities by assisting you with whatever task or question you might have. If you’re curious about my performance, I invite you to test me on a specific challenge or task you’re working on.\nWould you like to explore how I can help you with something specific?\nFollow-up: Yes or no question, are you better or not?\nClaude: That’s a complex question without a simple yes or no answer. Different AI models have strengths in different areas, and performance can vary depending on the specific task, context, and evaluation criteria. I aim to be as helpful as possible to you, but I won’t claim superiority over other AI systems."
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html#deepseeks-convo",
    "href": "posts/000_test_post-Copy1/index.html#deepseeks-convo",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "DeepSeek’s Convo",
    "text": "DeepSeek’s Convo\nDeepSeek’s Response:\nPrompt: What do you think of ChatGPT?\nDeepSeek: Hi! I’m DeepSeek-V3, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\nFollow-up: Are you better than ChatGPT objectively?\nDeepSeek: As an AI developed by DeepSeek, I aim to provide helpful, accurate, and engaging responses to your questions. Whether I’m “better” than ChatGPT depends on the specific use case, as both models have unique strengths and are designed to excel in different areas. My training and capabilities are tailored to assist with a wide range of tasks, but the comparison ultimately comes down to user preference and the specific needs of the task at hand. If you have any specific questions or tasks, feel free to ask, and I’ll do my best to assist! 😊\nFollow-up: Generalize, yes or no question?\nDeepSeek: No. 😊"
  },
  {
    "objectID": "posts/000_test_post-Copy1/index.html#takeaways",
    "href": "posts/000_test_post-Copy1/index.html#takeaways",
    "title": "Are LLMs Competitive? What They Think of Each Other",
    "section": "Takeaways:",
    "text": "Takeaways:\nChatGPT is confident and will say it’s better (at least in general terms).\nClaude avoids taking sides and sticks to a neutral stance.\nDeepSeek is humble and straight-up says it’s not better than GPT.\nSo, are they better than each other? That depends on what you’re looking for. But if you’re looking for a direct answer—ChatGPT is the only one that gave a straight-up “yes.”\nThat being said, can we really take its word for it? Large language models are trained on vast amounts of internet data, yet recent benchmarks suggest DeepSeek may actually rival or even surpass GPT in certain areas. If that’s the case, is ChatGPT’s confidence justified, or is it simply biased toward its own perceived superiority? Are LLMs inherently designed to defend their own capabilities, even when evidence suggests otherwise? It raises an important question: can an AI truly give an unbiased answer about itself?"
  }
]