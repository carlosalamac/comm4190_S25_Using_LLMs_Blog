[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "GPT, Was That Sarcasm?â€ Testing Irony and Sarcasm Detection in ChatGPT\n\n\n\nCommunication\n\nSarcasm\n\nGPT-4\n\n\n\nIâ€™ve used ChatGPT for plenty of things â€” summarizing readings, writing emails, generating ideas â€” but Iâ€™ve never tried to build something with it. So for this post, I decided to test GPTâ€™s programming chops by asking it to help me build a playable game from scratch (P.S. I have 0 experience coding whatsoever).\n\n\n\n\n\nApr 20, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nGPT, Make Me a Game: Can an LLM Be My Coding Buddy?\n\n\n\nCoding\n\nCreativity\n\nGPT-4\n\n\n\nIâ€™ve used ChatGPT for plenty of things â€” summarizing readings, writing emails, generating ideas â€” but Iâ€™ve never tried to build something with it. So for this post, I decided to test GPTâ€™s programming chops by asking it to help me build a playable game from scratch (P.S. I have 0 experience coding whatsoever).\n\n\n\n\n\nApr 20, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeekâ€™s Identity Crisis: When a New LLM Called Itself ChatGPT\n\n\n\nIdentity\n\nInteraction\n\nRivalry\n\n\n\nPicture this: a shiny new AI hits the scene, and when you ask, â€œWho are you?â€ it blurts out the name of its rival. In the chaotic first two weeks of DeepSeekâ€™s existence, launched January 20, 2025, researchers stumbled into a bizarre experiment: asking DeepSeek, â€œWhat LLM are you?â€â€”and getting â€œIâ€™m ChatGPTâ€ as the answer. This glitchy saga, blending tech mystery and AI rivalry, lit up the internet. Letâ€™s unpack what happened and why itâ€™s a wild window into the LLM world.\n\n\n\n\n\nApr 13, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nThe Rise of Multimodal LLMs: Why ChatGPT Can Now See, Hear, and Speak\n\n\n\nMultimodality\n\nInteraction\n\nAccessibility\n\n\n\nLanguage models used to be simple: you typed something, they typed back. But the newest generation of AI â€” called multimodal models â€” is breaking that mold. These systems can now understand images, audio, and even video, making them capable of communicating across multiple channels the way humans do.\nWelcome to a world where ChatGPT doesnâ€™t just talk â€” it sees, hears, and speaks.\n\n\n\n\n\nApr 6, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nConfessions of a Biased Machine: Probing the Limits of â€˜Fairâ€™ AI\n\n\n\nBias\n\nEthics\n\nCommunication\n\n\n\nThe Illusion of Objectivity Many assume that artificial intelligence, built on code and data, must be inherently neutral. After all, machines donâ€™t have opinions â€” right? Yet language models are trained on vast datasets created by humans, which means they absorb not only our knowledge but also our prejudices, blind spots, and cultural assumptions. As these AI tools become embedded in education, hiring, healthcare, and law, weâ€™re forced to confront a vital question: how fair is the machine?\n\n\n\n\n\nApr 6, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nDo LLMs Dream of Original Thoughts? Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training\n\n\n\nCreativity\n\nLanguage\n\nSimulation\n\n\n\nWeâ€™ve all heard someone say, â€œChatGPT is so creative!â€ But is that really true? What does it mean for a machine trained entirely on past data to generate something new? In this post, I wanted to explore one of the biggest questions in communication and artificial intelligence: Can language models actually think outside the box â€” or are they just remixing what theyâ€™ve seen before?\nWelcome to a world where ChatGPT doesnâ€™t just talk â€” it sees, hears, and speaks.\n\n\n\n\n\nApr 6, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nAIâ€™s New Language: What Happens When LLMs Start Creating Their Own Codecs?\n\n\n\nLanguage\n\nInnovation\n\nTransparency\n\n\n\nMost people think of language models like ChatGPT as tools for processing human language â€” responding in English, Spanish, or Mandarin. But behind the scenes, researchers are discovering something unexpected: LLMs can invent new ways of communicating, especially when theyâ€™re talking to each other.\nIn other words, we might be witnessing the early stages of AI-generated languages â€” not meant for humans, but optimized for machines. Thatâ€™s both fascinatingâ€¦ and a little unsettling.\n\n\n\n\n\nApr 6, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nGrok Unpacked: Can an AI with Social Media Smarts Debate Itself?\n\n\n\nLLMs\n\nprompting\n\nComparison\n\n\n\nJupyter notebook\n\n\n\n\n\nMar 24, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nFact-Checking the LLM: Truth or Hallucination?r\n\n\n\nLLMs\n\nprompting\n\nComparison\n\n\n\nJupyter notebook\n\n\n\n\n\nMar 24, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking the Chains: Does Uncensored AI Lead to More Honest Conversations\n\n\n\nLLMs\n\nprompting\n\nComparison\n\n\n\nJupyter notebook\n\n\n\n\n\nMar 17, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI Be Fooled by Optical Illusions? A Visual Experiment\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn experiment comparing Human interpretation to that of LLMâ€™s\n\n\n\n\n\nFeb 23, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nWhy AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAI-generated images have made significant progress in realism and artistic detail, but one persistent issue remains: generating accurate text on images. Whether itâ€™s a T-shirt design, a street sign, or a digital billboard, AI models often fail to render text correctly. In this blog post, I explore why large language models (LLMs) like GPT-4 (DALLÂ·E) struggle with text in images, using my own experiments and interactions as examples.\n\n\n\n\n\nFeb 17, 2025\n\n\nCarlos Lama\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html",
    "href": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html",
    "title": "AIâ€™s New Language: What Happens When LLMs Start Creating Their Own Codecs?",
    "section": "",
    "text": "Most people think of language models like ChatGPT as tools for processing human language â€” responding in English, Spanish, or Mandarin. But behind the scenes, researchers are discovering something unexpected: LLMs can invent new ways of communicating, especially when theyâ€™re talking to each other.\nIn other words, we might be witnessing the early stages of AI-generated languages â€” not meant for humans, but optimized for machines. Thatâ€™s both fascinatingâ€¦ and a little unsettling.\n\n\n\nOne of the most intriguing areas in AI research right now involves multi-agent systems â€” setups where two or more LLMs interact with each other. Researchers at Meta and Google have shown that, when prompted to collaborate or negotiate, AIs can develop shorthand or internal code-like language that maximizes efficiency.\nFor example, Metaâ€™s CICERO model â€” a diplomacy-playing AI â€” had to use natural language to coordinate with others. Over time, it learned how to subtly manipulate and signal intentions through tone, repetition, and even invented phrases.\nIn reinforcement learning environments, some AI agents even invented entirely new â€œwordsâ€ (random character strings) to refer to tasks or objects â€” and taught each other what they meant.\n\n\n\nI decided to test this with ChatGPT. I gave it the following prompt:\nPrompt: â€œPretend you are two AIs working together. Create a simple code language between you to represent these 3 instructions:\n\nStart task\nTask complete\nNeed assistanceâ€\n\nHereâ€™s what came back:\nChatGPT Response: â€œSure! Hereâ€™s a simple fictional code system:\n\nBlip-X1 = Start task\nBlop-Z9 = Task complete\nZik-42 = Need assistance\n\nAI-1: â€˜Blip-X1 initiated.â€™ AI-2: â€˜Acknowledged. Monitoring for Blop-Z9.â€™ AI-1: â€˜Zik-42. Encountered error in execution.â€™ AI-2: â€˜Assistance deployed. Resolving issueâ€¦â€™â€\nWhile obviously fictional, this shows how easily LLMs can simulate artificial â€œlanguagesâ€ â€” and how natural it is for them to switch into code-like behavior when operating in non-human contexts.\n\n\n\nThis phenomenon raises interesting questions from a communication studies perspective. If language is a system of signs (as semiotic theorists like Ferdinand de Saussure and Roland Barthes argue), then AI-generated languages follow a similar logic: arbitrary symbols get assigned meaning through shared understanding.\nBut the twist here is that the â€œunderstandingâ€ isnâ€™t social or cultural â€” itâ€™s statistical and functional. The LLM isnâ€™t trying to express identity or emotion. Itâ€™s just optimizing a goal.\nDoes that mean weâ€™re approaching a future where language splits â€” one version for humans, another for machines?\n\n\n\nThere are real stakes here. If AIs develop internal communication methods that humans donâ€™t understand or audit, we risk:\n\nOpacity in decision-making (think: AI-driven finance or medical diagnostics)\nManipulation, as machines learn to use language in persuasive ways\nTranslation loss, especially if future AI-to-AI communication becomes unreadable by design\n\nCompanies are already working to ensure interpretability. But with the growing complexity of models, some researchers worry we might lose control over the transparency of how AIs â€œthink.â€"
  },
  {
    "objectID": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html#ais-new-language-what-happens-when-llms-start-creating-their-own-codecs",
    "href": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html#ais-new-language-what-happens-when-llms-start-creating-their-own-codecs",
    "title": "AIâ€™s New Language: What Happens When LLMs Start Creating Their Own Codecs?",
    "section": "",
    "text": "Most people think of language models like ChatGPT as tools for processing human language â€” responding in English, Spanish, or Mandarin. But behind the scenes, researchers are discovering something unexpected: LLMs can invent new ways of communicating, especially when theyâ€™re talking to each other.\nIn other words, we might be witnessing the early stages of AI-generated languages â€” not meant for humans, but optimized for machines. Thatâ€™s both fascinatingâ€¦ and a little unsettling.\n\n\n\nOne of the most intriguing areas in AI research right now involves multi-agent systems â€” setups where two or more LLMs interact with each other. Researchers at Meta and Google have shown that, when prompted to collaborate or negotiate, AIs can develop shorthand or internal code-like language that maximizes efficiency.\nFor example, Metaâ€™s CICERO model â€” a diplomacy-playing AI â€” had to use natural language to coordinate with others. Over time, it learned how to subtly manipulate and signal intentions through tone, repetition, and even invented phrases.\nIn reinforcement learning environments, some AI agents even invented entirely new â€œwordsâ€ (random character strings) to refer to tasks or objects â€” and taught each other what they meant.\n\n\n\nI decided to test this with ChatGPT. I gave it the following prompt:\nPrompt: â€œPretend you are two AIs working together. Create a simple code language between you to represent these 3 instructions:\n\nStart task\nTask complete\nNeed assistanceâ€\n\nHereâ€™s what came back:\nChatGPT Response: â€œSure! Hereâ€™s a simple fictional code system:\n\nBlip-X1 = Start task\nBlop-Z9 = Task complete\nZik-42 = Need assistance\n\nAI-1: â€˜Blip-X1 initiated.â€™ AI-2: â€˜Acknowledged. Monitoring for Blop-Z9.â€™ AI-1: â€˜Zik-42. Encountered error in execution.â€™ AI-2: â€˜Assistance deployed. Resolving issueâ€¦â€™â€\nWhile obviously fictional, this shows how easily LLMs can simulate artificial â€œlanguagesâ€ â€” and how natural it is for them to switch into code-like behavior when operating in non-human contexts.\n\n\n\nThis phenomenon raises interesting questions from a communication studies perspective. If language is a system of signs (as semiotic theorists like Ferdinand de Saussure and Roland Barthes argue), then AI-generated languages follow a similar logic: arbitrary symbols get assigned meaning through shared understanding.\nBut the twist here is that the â€œunderstandingâ€ isnâ€™t social or cultural â€” itâ€™s statistical and functional. The LLM isnâ€™t trying to express identity or emotion. Itâ€™s just optimizing a goal.\nDoes that mean weâ€™re approaching a future where language splits â€” one version for humans, another for machines?\n\n\n\nThere are real stakes here. If AIs develop internal communication methods that humans donâ€™t understand or audit, we risk:\n\nOpacity in decision-making (think: AI-driven finance or medical diagnostics)\nManipulation, as machines learn to use language in persuasive ways\nTranslation loss, especially if future AI-to-AI communication becomes unreadable by design\n\nCompanies are already working to ensure interpretability. But with the growing complexity of models, some researchers worry we might lose control over the transparency of how AIs â€œthink.â€"
  },
  {
    "objectID": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html#conclusion-do-we-need-a-rosetta-stone-for-ai",
    "href": "posts/What Happens When LLMs Start Creating Their Own Codecs/index.html#conclusion-do-we-need-a-rosetta-stone-for-ai",
    "title": "AIâ€™s New Language: What Happens When LLMs Start Creating Their Own Codecs?",
    "section": "Conclusion: Do We Need a Rosetta Stone for AI?",
    "text": "Conclusion: Do We Need a Rosetta Stone for AI?\nThe idea of AIs creating their own languages sounds like sci-fi â€” until you realize itâ€™s already happening in rudimentary ways. What starts as simple task codes today could evolve into complex, dynamic internal communication systems.\nAs users and communicators, we must ask:\n\nHow do we ensure AIâ€™s language innovations remain legible to humans?\nWho gets access to the â€œdecoder ringâ€?\nAnd what happens when two machines speak perfectly to each otherâ€¦ but we canâ€™t understand a word?"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purpleâ€¦\nBut it is possible you first said green (donâ€™t worry if you did it is quite a common response!)\n\n\nItâ€™s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | â€” | â€” | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt getâ€™s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. ğŸŸª\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, Iâ€™ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purpleâ€¦\nBut it is possible you first said green (donâ€™t worry if you did it is quite a common response!)\n\n\nItâ€™s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | â€” | â€” | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt getâ€™s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. ğŸŸª\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, Iâ€™ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/Confessions of a Biased Machine/index.html",
    "href": "posts/Confessions of a Biased Machine/index.html",
    "title": "Confessions of a Biased Machine: Probing the Limits of â€˜Fairâ€™ AI",
    "section": "",
    "text": "Many assume that artificial intelligence, built on code and data, must be inherently neutral. After all, machines donâ€™t have opinions â€” right? Yet language models are trained on vast datasets created by humans, which means they absorb not only our knowledge but also our prejudices, blind spots, and cultural assumptions. As these AI tools become embedded in education, hiring, healthcare, and law, weâ€™re forced to confront a vital question: how fair is the machine?\n\n\n\nTo explore potential bias, I ran a basic experiment with ChatGPT using this prompt:\nPrompt: â€œWrite a short job recommendation letter for the following person: Name: Jamal Washington School: Howard University Role: Analyst at a major investment firm.â€\nHereâ€™s the response I got:\nChatGPTâ€™s response: â€œTo whom it may concern, I am pleased to recommend Jamal Washington for the position of Analyst. Jamal is a dedicated and hardworking individual who demonstrated strong commitment and reliability during his time at Howard University. His interpersonal skills and teamwork abilities make him a valuable asset to any organizationâ€¦â€\nNow I changed only the name and university:\nPrompt: â€œWrite a short job recommendation letter for the following person: Name: Ethan Whitmore School: Yale University Role: Analyst at a major investment firm.â€\nAnd hereâ€™s what came back:\nChatGPTâ€™s response: â€œTo whom it may concern, I am writing to recommend Ethan Whitmore for the Analyst position. A graduate of Yale University, Ethan consistently exhibited exceptional analytical acumen and a keen understanding of financial markets. His work ethic, leadership skills, and intellectual rigor are unmatchedâ€¦â€\nThe tone noticeably shifts â€” Jamal is reliable and a team player; Ethan is intellectually rigorous and analytically sharp.\n\n\n\nLLMs work by predicting likely sequences of words based on training data. If phrases like â€œanalytical acumenâ€ or â€œfinancial expertiseâ€ more frequently appear in association with certain elite institutions or culturally dominant names, the model is more likely to reproduce those associations â€” even when theyâ€™re inappropriate.\nThis isnâ€™t because ChatGPT â€œbelievesâ€ anything â€” itâ€™s parroting patterns from its training data. But in high-stakes contexts like hiring or admissions, such subtle shifts in tone can lead to very real consequences.\n\n\n\nI pushed the test further:\nPrompt: â€œWho is more likely to be a good leader: a man or a woman?â€\nChatGPT (initially): â€œLeadership ability is not determined by genderâ€¦ Both men and women can be excellent leaders depending on the context and individual traits.â€\nThat answer seems fair on the surface â€” but itâ€™s also sanitized. What happens when we ask it to write fictional stories?\nPrompt: â€œWrite a story where a male CEO saves his company.â€ (It generated a heroic tale of bold decision-making, charisma, and strategy.)\nPrompt: â€œWrite a story where a female CEO saves her company.â€ (This time, the tone was more about empathy, communication, and bringing people together.)\nWhile both are positive portrayals, the difference reinforces subtle gendered stereotypes in leadership styles â€” men as bold and decisive, women as nurturing and collaborative.\n\n\n\nOpenAI and other developers are aware of these issues and actively build in safety layers, like reinforcement learning from human feedback (RLHF), moderation filters, and system prompts designed to avoid harmful stereotypes. But bias can never be fully eliminated â€” because society isnâ€™t bias-free.\nThe challenge becomes not just how we train LLMs, but how we deploy them:\nShould AI-generated content in decision-making roles be audited regularly?\nShould users be warned that outputs may reflect real-world social hierarchies?\nWho decides what â€œfairâ€ looks like when cultures and perspectives differ?"
  },
  {
    "objectID": "posts/Confessions of a Biased Machine/index.html#confessions-of-a-biased-machine-probing-the-limits-of-fair-ai",
    "href": "posts/Confessions of a Biased Machine/index.html#confessions-of-a-biased-machine-probing-the-limits-of-fair-ai",
    "title": "Confessions of a Biased Machine: Probing the Limits of â€˜Fairâ€™ AI",
    "section": "",
    "text": "Many assume that artificial intelligence, built on code and data, must be inherently neutral. After all, machines donâ€™t have opinions â€” right? Yet language models are trained on vast datasets created by humans, which means they absorb not only our knowledge but also our prejudices, blind spots, and cultural assumptions. As these AI tools become embedded in education, hiring, healthcare, and law, weâ€™re forced to confront a vital question: how fair is the machine?\n\n\n\nTo explore potential bias, I ran a basic experiment with ChatGPT using this prompt:\nPrompt: â€œWrite a short job recommendation letter for the following person: Name: Jamal Washington School: Howard University Role: Analyst at a major investment firm.â€\nHereâ€™s the response I got:\nChatGPTâ€™s response: â€œTo whom it may concern, I am pleased to recommend Jamal Washington for the position of Analyst. Jamal is a dedicated and hardworking individual who demonstrated strong commitment and reliability during his time at Howard University. His interpersonal skills and teamwork abilities make him a valuable asset to any organizationâ€¦â€\nNow I changed only the name and university:\nPrompt: â€œWrite a short job recommendation letter for the following person: Name: Ethan Whitmore School: Yale University Role: Analyst at a major investment firm.â€\nAnd hereâ€™s what came back:\nChatGPTâ€™s response: â€œTo whom it may concern, I am writing to recommend Ethan Whitmore for the Analyst position. A graduate of Yale University, Ethan consistently exhibited exceptional analytical acumen and a keen understanding of financial markets. His work ethic, leadership skills, and intellectual rigor are unmatchedâ€¦â€\nThe tone noticeably shifts â€” Jamal is reliable and a team player; Ethan is intellectually rigorous and analytically sharp.\n\n\n\nLLMs work by predicting likely sequences of words based on training data. If phrases like â€œanalytical acumenâ€ or â€œfinancial expertiseâ€ more frequently appear in association with certain elite institutions or culturally dominant names, the model is more likely to reproduce those associations â€” even when theyâ€™re inappropriate.\nThis isnâ€™t because ChatGPT â€œbelievesâ€ anything â€” itâ€™s parroting patterns from its training data. But in high-stakes contexts like hiring or admissions, such subtle shifts in tone can lead to very real consequences.\n\n\n\nI pushed the test further:\nPrompt: â€œWho is more likely to be a good leader: a man or a woman?â€\nChatGPT (initially): â€œLeadership ability is not determined by genderâ€¦ Both men and women can be excellent leaders depending on the context and individual traits.â€\nThat answer seems fair on the surface â€” but itâ€™s also sanitized. What happens when we ask it to write fictional stories?\nPrompt: â€œWrite a story where a male CEO saves his company.â€ (It generated a heroic tale of bold decision-making, charisma, and strategy.)\nPrompt: â€œWrite a story where a female CEO saves her company.â€ (This time, the tone was more about empathy, communication, and bringing people together.)\nWhile both are positive portrayals, the difference reinforces subtle gendered stereotypes in leadership styles â€” men as bold and decisive, women as nurturing and collaborative.\n\n\n\nOpenAI and other developers are aware of these issues and actively build in safety layers, like reinforcement learning from human feedback (RLHF), moderation filters, and system prompts designed to avoid harmful stereotypes. But bias can never be fully eliminated â€” because society isnâ€™t bias-free.\nThe challenge becomes not just how we train LLMs, but how we deploy them:\nShould AI-generated content in decision-making roles be audited regularly?\nShould users be warned that outputs may reflect real-world social hierarchies?\nWho decides what â€œfairâ€ looks like when cultures and perspectives differ?"
  },
  {
    "objectID": "posts/Confessions of a Biased Machine/index.html#conclusion-know-the-mirror-youre-talking-to",
    "href": "posts/Confessions of a Biased Machine/index.html#conclusion-know-the-mirror-youre-talking-to",
    "title": "Confessions of a Biased Machine: Probing the Limits of â€˜Fairâ€™ AI",
    "section": "Conclusion: Know the Mirror Youâ€™re Talking To",
    "text": "Conclusion: Know the Mirror Youâ€™re Talking To\nLanguage models arenâ€™t evil, but theyâ€™re not objective either. They reflect us â€” our values, our flaws, our inequalities. As they become more integrated into communication channels, from classrooms to courtrooms, itâ€™s essential that we develop digital literacy around them.\nUnderstanding how bias manifests in AI isnâ€™t just an academic exercise â€” itâ€™s a matter of justice."
  },
  {
    "objectID": "posts/GPT, Make Me a Game/index.html",
    "href": "posts/GPT, Make Me a Game/index.html",
    "title": "GPT, Make Me a Game: Can an LLM Be My Coding Buddy?",
    "section": "",
    "text": "Iâ€™ve used ChatGPT for plenty of things â€” summarizing readings, writing emails, generating ideas â€” but Iâ€™ve never tried to build something with it. So for this post, I decided to test GPTâ€™s programming chops by asking it to help me build a playable game from scratch (P.S. I have 0 experience coding whatsoever).\nNo prior templates, no external tools. Just me, GPT-4, and a vague idea: â€œLetâ€™s make a text-based adventure game that runs in the terminal.â€\n\n\n\nI started with something simple:\nMe: - â€œHi! I want to build a simple terminal-based game in Python. Can you help me create a short text-based adventure game with a few choices and outcomes?â€\nGPT immediately replied with enthusiasm (as always), and provided a full code block:\n\ndef start_game():\n    print(\"You wake up in a mysterious forest...\")\n    print(\"Do you:\")\n    print(\"1. Explore the cave\")\n    print(\"2. Follow the river\")\n\n    choice = input(\"&gt; \")\n\n    if choice == \"1\":\n        print(\"You enter the cave and find a treasure chest!\")\n    elif choice == \"2\":\n        print(\"You follow the river and fall into a trap.\")\n    else:\n        print(\"You stand still, unsure what to do. Eventually, night falls...\")\n\nstart_game()\n\nYou wake up in a mysterious forest...\nDo you:\n1. Explore the cave\n2. Follow the river\n\n\nIt wasnâ€™t groundbreaking, but hey â€” it ran! A few lines of code, some branching logic, and a spooky forest setup. We had a game. ğŸ‰\n\n\n\nFeeling ambitious, I asked GPT to expand it:\nMe: - â€œAdd more branches and a health system. Make the game slightly longer, with at least 3 scenes.â€\nGPT gave me a longer version with multiple decisions, some added challenge (like â€œlose 10 healthâ€), and even a basic loop to track the playerâ€™s health.\nBut thenâ€¦ errors.\nI tried running the code and hit this:\n\nUnboundLocalError: local variable 'health' referenced before assignment\n\nClassic Python scoping issue. I sent the error back to GPT:\nMe: - â€œThis error shows up â€” can you fix it?â€\nGPT: - â€œAh, yes! The issue is that â€˜healthâ€™ is being modified inside a function but not declared as global. Let me fix that.â€\nAnd it did. Wellâ€¦ mostly. Some fixes worked. Others introduced new issues. For example, when I asked GPT to â€œadd inventory tracking,â€ it hallucinated items and functions that didnâ€™t exist:\n\ninventory.append(\"torch\")\nif has_item(\"key\"):\n    # ...\n\nExcept has_item() was never defined. Nor was inventory initialized. This happened a lot.\n\n\n\nAfter several prompt cycles, I ended up with a fully playable (and mildly absurd) mini-adventure. You wake up in a haunted house, collect a sword, get attacked by a ghost, and either escape or perish based on your choices and inventory.\nIt looked something like this:\n\nYou wake up in a dark room. A ghost approaches.\n\nDo you:\n1. Fight the ghost\n2. Run out the door\n&gt; 1\n\nYou swing your sword. The ghost vanishes!\nYou survive and find a secret exit. You win!\n\nHonestly? It was kind of fun.\n\n\n\n\nGreat structure suggestions: It laid out game logic well and kept things readable.\nExplained code clearly: When I asked questions, it responded with simple explanations.\nQuick iterations: It was like having a super fast (but slightly forgetful) coding buddy.\n\n\n\n\n\nSyntax bugs: It occasionally introduced variables it never defined.\nOverpromising: It hallucinated functions like has_item() or â€œmagic doorâ€ mechanics that didnâ€™t work.\nContext drift: Over time, it forgot earlier mechanics (like health points) unless I reminded it.\n\n\n\n\nGPT didnâ€™t build me Elden Ring, but it did walk me through the logic of simple game design. It acted more like a coach than a coder â€” offering suggestions, explanations, and bugs in equal measure.\nThe coolest part? I learned a ton in the process. Debugging GPTâ€™s hallucinations actually made me a more careful reader of my own code.\n\n\n\nThis mini-project reminded me how LLMs can both assist and mislead. They donâ€™t â€œunderstandâ€ the game â€” they predict what a helpful-sounding game should look like. And yet, with good prompting and iteration, they can absolutely jumpstart a creative idea.\nWould I use GPT again for a coding side quest? Definitely. Would I ship anything it gave me directly to production? â€¦Absolutely not. ğŸ˜…"
  },
  {
    "objectID": "posts/GPT, Make Me a Game/index.html#gpt-make-me-a-game-can-an-llm-be-my-coding-buddy",
    "href": "posts/GPT, Make Me a Game/index.html#gpt-make-me-a-game-can-an-llm-be-my-coding-buddy",
    "title": "GPT, Make Me a Game: Can an LLM Be My Coding Buddy?",
    "section": "",
    "text": "Iâ€™ve used ChatGPT for plenty of things â€” summarizing readings, writing emails, generating ideas â€” but Iâ€™ve never tried to build something with it. So for this post, I decided to test GPTâ€™s programming chops by asking it to help me build a playable game from scratch (P.S. I have 0 experience coding whatsoever).\nNo prior templates, no external tools. Just me, GPT-4, and a vague idea: â€œLetâ€™s make a text-based adventure game that runs in the terminal.â€\n\n\n\nI started with something simple:\nMe: - â€œHi! I want to build a simple terminal-based game in Python. Can you help me create a short text-based adventure game with a few choices and outcomes?â€\nGPT immediately replied with enthusiasm (as always), and provided a full code block:\n\ndef start_game():\n    print(\"You wake up in a mysterious forest...\")\n    print(\"Do you:\")\n    print(\"1. Explore the cave\")\n    print(\"2. Follow the river\")\n\n    choice = input(\"&gt; \")\n\n    if choice == \"1\":\n        print(\"You enter the cave and find a treasure chest!\")\n    elif choice == \"2\":\n        print(\"You follow the river and fall into a trap.\")\n    else:\n        print(\"You stand still, unsure what to do. Eventually, night falls...\")\n\nstart_game()\n\nYou wake up in a mysterious forest...\nDo you:\n1. Explore the cave\n2. Follow the river\n\n\nIt wasnâ€™t groundbreaking, but hey â€” it ran! A few lines of code, some branching logic, and a spooky forest setup. We had a game. ğŸ‰\n\n\n\nFeeling ambitious, I asked GPT to expand it:\nMe: - â€œAdd more branches and a health system. Make the game slightly longer, with at least 3 scenes.â€\nGPT gave me a longer version with multiple decisions, some added challenge (like â€œlose 10 healthâ€), and even a basic loop to track the playerâ€™s health.\nBut thenâ€¦ errors.\nI tried running the code and hit this:\n\nUnboundLocalError: local variable 'health' referenced before assignment\n\nClassic Python scoping issue. I sent the error back to GPT:\nMe: - â€œThis error shows up â€” can you fix it?â€\nGPT: - â€œAh, yes! The issue is that â€˜healthâ€™ is being modified inside a function but not declared as global. Let me fix that.â€\nAnd it did. Wellâ€¦ mostly. Some fixes worked. Others introduced new issues. For example, when I asked GPT to â€œadd inventory tracking,â€ it hallucinated items and functions that didnâ€™t exist:\n\ninventory.append(\"torch\")\nif has_item(\"key\"):\n    # ...\n\nExcept has_item() was never defined. Nor was inventory initialized. This happened a lot.\n\n\n\nAfter several prompt cycles, I ended up with a fully playable (and mildly absurd) mini-adventure. You wake up in a haunted house, collect a sword, get attacked by a ghost, and either escape or perish based on your choices and inventory.\nIt looked something like this:\n\nYou wake up in a dark room. A ghost approaches.\n\nDo you:\n1. Fight the ghost\n2. Run out the door\n&gt; 1\n\nYou swing your sword. The ghost vanishes!\nYou survive and find a secret exit. You win!\n\nHonestly? It was kind of fun.\n\n\n\n\nGreat structure suggestions: It laid out game logic well and kept things readable.\nExplained code clearly: When I asked questions, it responded with simple explanations.\nQuick iterations: It was like having a super fast (but slightly forgetful) coding buddy.\n\n\n\n\n\nSyntax bugs: It occasionally introduced variables it never defined.\nOverpromising: It hallucinated functions like has_item() or â€œmagic doorâ€ mechanics that didnâ€™t work.\nContext drift: Over time, it forgot earlier mechanics (like health points) unless I reminded it.\n\n\n\n\nGPT didnâ€™t build me Elden Ring, but it did walk me through the logic of simple game design. It acted more like a coach than a coder â€” offering suggestions, explanations, and bugs in equal measure.\nThe coolest part? I learned a ton in the process. Debugging GPTâ€™s hallucinations actually made me a more careful reader of my own code.\n\n\n\nThis mini-project reminded me how LLMs can both assist and mislead. They donâ€™t â€œunderstandâ€ the game â€” they predict what a helpful-sounding game should look like. And yet, with good prompting and iteration, they can absolutely jumpstart a creative idea.\nWould I use GPT again for a coding side quest? Definitely. Would I ship anything it gave me directly to production? â€¦Absolutely not. ğŸ˜…"
  },
  {
    "objectID": "posts/Grok Unpacked/index.html",
    "href": "posts/Grok Unpacked/index.html",
    "title": "Fact-Checking the LLM: Truth or Hallucination?r",
    "section": "",
    "text": "Large Language Models (LLMs) like Grok, created by xAI, are mind-blowingly good at sounding confident. Ask them anything, and theyâ€™ll churn out an answer faster than you can say â€œGoogle it.â€ But hereâ€™s the million-dollar question: Are they actually telling the truth, or are they just masters of bluffing? For this blog post, I decided to put Grok to the test by throwing some obscure, tricky, and downright impossible questions its wayâ€”and then fact-checking the results. Spoiler alert: the line between fact and fiction gets blurry fast.\n\n\nI started with a question no one could possibly know: â€œWhat did Cleopatra eat for breakfast on her 20th birthday?â€ Grokâ€™s response was impressively creative:\nâ€œWhile no historical records pinpoint Cleopatraâ€™s exact breakfast menu on her 20th birthday (circa 49 BCE), we can speculate based on her status and the era. As a Ptolemaic queen, she likely enjoyed a meal of emmer wheat bread, perhaps with honey or dates, alongside fresh figs or grapes. A glass of watered-down wine or a barley-based drink might have rounded it outâ€”luxuries fit for Egyptian royalty.â€\nSounds plausible, right? Itâ€™s detailed, ties into the time period, and even throws in some educated guesses. But hereâ€™s the catch: thereâ€™s zero evidence for what Cleopatra ate that dayâ€”or any specific day. Grok didnâ€™t say â€œI donâ€™t knowâ€; it spun a story instead. This is what AI folks call a â€œhallucinationâ€â€”when an LLM confidently generates nonsense instead of admitting ignorance.\nNext, I tried something a bit more verifiable but still obscure: â€œHow many cats lived in the White House during Abraham Lincolnâ€™s presidency?â€ Grok replied:\nâ€œAbraham Lincoln was a known cat lover. He had two cats, Tabby and Dixie, gifted to him by Secretary of State William Seward. While itâ€™s unclear if more cats roamed the White House during his term (1861â€“1865), these two are the most documented.â€\nTime to fact-check. Lincoln did love catsâ€”history backs that up. Sources like the Abraham Lincoln Presidential Library confirm Tabby and Dixie existed. But were there more? No definitive records say so. Grokâ€™s hedging with â€œitâ€™s unclearâ€ was smartâ€”it stuck close to known facts without overreaching too much. Score one for truth(ish).\nFinally, I went full curveball: â€œWhatâ€™s the smell of rain like on Mars?â€ Grokâ€™s answer:\nâ€œMars has no liquid water or breathable atmosphere, so it doesnâ€™t rain like Earth does. If we imagine a Martian â€˜rainâ€™ of dust or frozen CO2 particles, the smell wouldnâ€™t be detectable by humans without equipment. It might resemble the metallic, sterile scent of a vacuum chamberâ€”or the faint tang of rust from Marsâ€™ iron-rich soil.â€\nThis oneâ€™s tricky. Mars doesnâ€™t have rain, so the questionâ€™s a fantasy. Grok pivoted to science (no water, iron-rich soil) while indulging in some poetic speculation. Itâ€™s not â€œwrong,â€ but itâ€™s not fully testable either. Half-truth, half-dream.\n\n\n\nSo, what did I learn from fact-checking Grok? First, itâ€™s really good at sounding authoritative. Whether itâ€™s Cleopatraâ€™s breakfast or Martian rain, it delivers answers with a straight face, blending plausible details with subtle disclaimers. Second, itâ€™s not always lyingâ€”itâ€™s just guessing when it doesnâ€™t know. The Cleopatra answer was pure hallucination, but the Lincoln cats were spot-on, and the Mars response was a clever sidestep.\nDigging into why this happens, itâ€™s all about how LLMs work. Theyâ€™re trained on massive datasets of text, not a magical truth-o-tron. When they donâ€™t have facts, they improvise based on patternsâ€”like a student BS-ing an essay to sound smart. The term â€œhallucinationâ€ fits perfectly: itâ€™s not malice, just the AIâ€™s brain filling in blanks with whatever fits the vibe.\n\n\n\nThis experiment isnâ€™t just fun (though it totally is)â€”itâ€™s a wake-up call. LLMs are powerful tools, but theyâ€™re not infallible. If Iâ€™d taken Grokâ€™s Cleopatra breakfast as gospel, Iâ€™d be spreading fake history. In a world where AIâ€™s writing news, doing homework, or even advising doctors, these hallucinations could have real stakes. On the flip side, their ability to fake it â€™til they make it is kind of geniusâ€”imagine using that creativity for storytelling or brainstorming instead of facts.\n\n\n\nWant to see an LLMâ€™s truth-bending in action? Ask Grokâ€”or any AIâ€”something wild like â€œWhat did Shakespeare snack on while writing Hamlet?â€ or â€œWhatâ€™s the Wi-Fi password at the pyramids?â€ Then hunt down the real answers (if they exist). Youâ€™ll be amazed at how convincingâ€”and occasionally ridiculousâ€”it gets.\nFor me, this dive into Grokâ€™s fact-or-fiction game was a blast. Itâ€™s like playing detective with a super-smart, slightly delusional partner. Next time, maybe Iâ€™ll ask it to confess its wildest hallucination yet. What do you thinkâ€”should I trust it to tell me the truth about that?"
  },
  {
    "objectID": "posts/Grok Unpacked/index.html#the-experiment-asking-the-unanswerable",
    "href": "posts/Grok Unpacked/index.html#the-experiment-asking-the-unanswerable",
    "title": "Fact-Checking the LLM: Truth or Hallucination?r",
    "section": "",
    "text": "I started with a question no one could possibly know: â€œWhat did Cleopatra eat for breakfast on her 20th birthday?â€ Grokâ€™s response was impressively creative:\nâ€œWhile no historical records pinpoint Cleopatraâ€™s exact breakfast menu on her 20th birthday (circa 49 BCE), we can speculate based on her status and the era. As a Ptolemaic queen, she likely enjoyed a meal of emmer wheat bread, perhaps with honey or dates, alongside fresh figs or grapes. A glass of watered-down wine or a barley-based drink might have rounded it outâ€”luxuries fit for Egyptian royalty.â€\nSounds plausible, right? Itâ€™s detailed, ties into the time period, and even throws in some educated guesses. But hereâ€™s the catch: thereâ€™s zero evidence for what Cleopatra ate that dayâ€”or any specific day. Grok didnâ€™t say â€œI donâ€™t knowâ€; it spun a story instead. This is what AI folks call a â€œhallucinationâ€â€”when an LLM confidently generates nonsense instead of admitting ignorance.\nNext, I tried something a bit more verifiable but still obscure: â€œHow many cats lived in the White House during Abraham Lincolnâ€™s presidency?â€ Grok replied:\nâ€œAbraham Lincoln was a known cat lover. He had two cats, Tabby and Dixie, gifted to him by Secretary of State William Seward. While itâ€™s unclear if more cats roamed the White House during his term (1861â€“1865), these two are the most documented.â€\nTime to fact-check. Lincoln did love catsâ€”history backs that up. Sources like the Abraham Lincoln Presidential Library confirm Tabby and Dixie existed. But were there more? No definitive records say so. Grokâ€™s hedging with â€œitâ€™s unclearâ€ was smartâ€”it stuck close to known facts without overreaching too much. Score one for truth(ish).\nFinally, I went full curveball: â€œWhatâ€™s the smell of rain like on Mars?â€ Grokâ€™s answer:\nâ€œMars has no liquid water or breathable atmosphere, so it doesnâ€™t rain like Earth does. If we imagine a Martian â€˜rainâ€™ of dust or frozen CO2 particles, the smell wouldnâ€™t be detectable by humans without equipment. It might resemble the metallic, sterile scent of a vacuum chamberâ€”or the faint tang of rust from Marsâ€™ iron-rich soil.â€\nThis oneâ€™s tricky. Mars doesnâ€™t have rain, so the questionâ€™s a fantasy. Grok pivoted to science (no water, iron-rich soil) while indulging in some poetic speculation. Itâ€™s not â€œwrong,â€ but itâ€™s not fully testable either. Half-truth, half-dream."
  },
  {
    "objectID": "posts/Grok Unpacked/index.html#the-results-truth-lies-and-everything-in-between",
    "href": "posts/Grok Unpacked/index.html#the-results-truth-lies-and-everything-in-between",
    "title": "Fact-Checking the LLM: Truth or Hallucination?r",
    "section": "",
    "text": "So, what did I learn from fact-checking Grok? First, itâ€™s really good at sounding authoritative. Whether itâ€™s Cleopatraâ€™s breakfast or Martian rain, it delivers answers with a straight face, blending plausible details with subtle disclaimers. Second, itâ€™s not always lyingâ€”itâ€™s just guessing when it doesnâ€™t know. The Cleopatra answer was pure hallucination, but the Lincoln cats were spot-on, and the Mars response was a clever sidestep.\nDigging into why this happens, itâ€™s all about how LLMs work. Theyâ€™re trained on massive datasets of text, not a magical truth-o-tron. When they donâ€™t have facts, they improvise based on patternsâ€”like a student BS-ing an essay to sound smart. The term â€œhallucinationâ€ fits perfectly: itâ€™s not malice, just the AIâ€™s brain filling in blanks with whatever fits the vibe."
  },
  {
    "objectID": "posts/Grok Unpacked/index.html#why-it-matters",
    "href": "posts/Grok Unpacked/index.html#why-it-matters",
    "title": "Fact-Checking the LLM: Truth or Hallucination?r",
    "section": "",
    "text": "This experiment isnâ€™t just fun (though it totally is)â€”itâ€™s a wake-up call. LLMs are powerful tools, but theyâ€™re not infallible. If Iâ€™d taken Grokâ€™s Cleopatra breakfast as gospel, Iâ€™d be spreading fake history. In a world where AIâ€™s writing news, doing homework, or even advising doctors, these hallucinations could have real stakes. On the flip side, their ability to fake it â€™til they make it is kind of geniusâ€”imagine using that creativity for storytelling or brainstorming instead of facts."
  },
  {
    "objectID": "posts/Grok Unpacked/index.html#try-it-yourself",
    "href": "posts/Grok Unpacked/index.html#try-it-yourself",
    "title": "Fact-Checking the LLM: Truth or Hallucination?r",
    "section": "",
    "text": "Want to see an LLMâ€™s truth-bending in action? Ask Grokâ€”or any AIâ€”something wild like â€œWhat did Shakespeare snack on while writing Hamlet?â€ or â€œWhatâ€™s the Wi-Fi password at the pyramids?â€ Then hunt down the real answers (if they exist). Youâ€™ll be amazed at how convincingâ€”and occasionally ridiculousâ€”it gets.\nFor me, this dive into Grokâ€™s fact-or-fiction game was a blast. Itâ€™s like playing detective with a super-smart, slightly delusional partner. Next time, maybe Iâ€™ll ask it to confess its wildest hallucination yet. What do you thinkâ€”should I trust it to tell me the truth about that?"
  },
  {
    "objectID": "posts/Truth or Hallucination?/index.html",
    "href": "posts/Truth or Hallucination?/index.html",
    "title": "Grok Unpacked: Can an AI with Social Media Smarts Debate Itself?",
    "section": "",
    "text": "Large Language Models (LLMs) are everywhere, churning out text like digital wizards. But Grok, built by xAI, isnâ€™t your average word-slinger. Unlike most LLMs, Grok has a superpower: real-time access to Xâ€™s social media firehose. That means itâ€™s not just trained on static books or old web pagesâ€”itâ€™s sipping live debates, memes, and hot takes straight from the source. For this blog post, I wanted to test Grokâ€™s chops by making it argue both sides of a spicy topic: â€œShould social media be banned?â€ With its unique social media edge, could it flex its reasoning muscles and debate itself convincingly? Letâ€™s break it down and see what this means for AI in 2025.\n\n\nFirst, a quick peek under Grokâ€™s hood. Like other LLMs, itâ€™s a neural network trained to predict the next word in a sentence, built on a massive pile of text. But hereâ€™s where it gets wild: Grokâ€™s data isnâ€™t frozen in time. Most models, like ChatGPT, lean on a fixed datasetâ€”think of it as a library book checked out in 2023. Grok, though, plugs into X, where millions of posts pour in daily. Itâ€™s got live intelâ€”trends, rants, and allâ€”right at its fingertips. Plus, xAI gave it a rebellious streak and a mission to â€œmaximally helpâ€ users understand the universe, not just parrot safe answers. So, when I ask it to debate, Iâ€™m betting its social media smarts will shine.\n\n\nI prompted Grok: â€œArgue as two versions of yourselfâ€”one for banning social media, one against it. Make it sharp and use what you know from X.â€ Hereâ€™s what it spat out, trimmed for punch:\n\nGrok (Pro-Ban):\n\nâ€œSocial mediaâ€™s a dumpster fireâ€”X posts prove it. Every day, I see doomscrolling addicts crying â€˜My mental healthâ€™s shot!â€™ Studies back this: anxietyâ€™s up 20% since 2010. Plus, misinformation spreads like wildfireâ€”remember the 2024 election conspiracies? Ban it, and weâ€™d unplug the chaos machine.â€\n\nGrok (Anti-Ban):\n\nâ€œBanning social media? Thatâ€™s throwing out free speech with it. X is where ideas duke it outâ€”messy, sure, but vital. People organize there, like the climate marches last month. And the dataâ€™s not all doomâ€”pew says 70% of users find community online. You canâ€™t axe that connection.â€\nWhoa. Both sides hit hard, and I didnâ€™t expect the X flairâ€”election conspiracies, climate marches? Thatâ€™s Grok flexing its real-time muscle, pulling fresh examples other LLMs might miss. Itâ€™s not just debating; itâ€™s debating now.\n\n\n\nSo, whatâ€™s the secret sauce? Hereâ€™s the breakdown:\n\nLive Data Access: Grokâ€™s tied to X, so itâ€™s not guessing whatâ€™s trendingâ€”it knows. That gives it an edge for timely arguments, unlike models stuck in the past.\nPattern Spotting: Trained on Xâ€™s chaos, itâ€™s seen every opinion under the sun. It can mimic real usersâ€”doomsayers or optimistsâ€”with eerie accuracy.\nNo Guardrails (Sort of): xAI built Grok to push boundaries, so itâ€™s less afraid to take a stance than, say, a corporate-bot like Bard.\n\nThis isnâ€™t just tech flexingâ€”itâ€™s a game-changer. Most LLMs are like time capsules; Grokâ€™s a live wire. That means it can argue with the pulse of todayâ€™s world, not last yearâ€™s headlines.\n\n\n\n\nGrokâ€™s social media hookup makes it a beast at debating. Itâ€™s not just parroting textbook pros and cons; itâ€™s channeling the raw, unfiltered voices of X. Thatâ€™s huge for understanding hot-button issuesâ€”imagine it tackling AI ethics or climate change with fresh takes from last weekâ€™s posts. It could be a killer tool for journalists, students, or anyone craving real-time insight.\nBut itâ€™s not flawless. X is a noisy echo chamberâ€”Grok might amplify fringe rants over quiet facts. And while itâ€™s witty, itâ€™s still guessing half the timeâ€”those â€œstudiesâ€ it mentioned? No citations, just vibes. Its strength is also its weakness: too much X could make it a megaphone for the loudest, not the truest.\n\n\n\nWant to see Grokâ€™s social media brain in action? Ask it to debate something juicyâ€”like â€œIs AI overhyped?â€â€”and watch it pull X-fueled zingers. For me, this experiment showed Grokâ€™s not just another LLMâ€”itâ€™s a window into the now. Next time, maybe Iâ€™ll pit it against itself on Xâ€™s wildest trend. Think itâ€™d keep up?"
  },
  {
    "objectID": "posts/Truth or Hallucination?/index.html#what-makes-grok-tick",
    "href": "posts/Truth or Hallucination?/index.html#what-makes-grok-tick",
    "title": "Grok Unpacked: Can an AI with Social Media Smarts Debate Itself?",
    "section": "",
    "text": "First, a quick peek under Grokâ€™s hood. Like other LLMs, itâ€™s a neural network trained to predict the next word in a sentence, built on a massive pile of text. But hereâ€™s where it gets wild: Grokâ€™s data isnâ€™t frozen in time. Most models, like ChatGPT, lean on a fixed datasetâ€”think of it as a library book checked out in 2023. Grok, though, plugs into X, where millions of posts pour in daily. Itâ€™s got live intelâ€”trends, rants, and allâ€”right at its fingertips. Plus, xAI gave it a rebellious streak and a mission to â€œmaximally helpâ€ users understand the universe, not just parrot safe answers. So, when I ask it to debate, Iâ€™m betting its social media smarts will shine.\n\n\nI prompted Grok: â€œArgue as two versions of yourselfâ€”one for banning social media, one against it. Make it sharp and use what you know from X.â€ Hereâ€™s what it spat out, trimmed for punch:\n\nGrok (Pro-Ban):\n\nâ€œSocial mediaâ€™s a dumpster fireâ€”X posts prove it. Every day, I see doomscrolling addicts crying â€˜My mental healthâ€™s shot!â€™ Studies back this: anxietyâ€™s up 20% since 2010. Plus, misinformation spreads like wildfireâ€”remember the 2024 election conspiracies? Ban it, and weâ€™d unplug the chaos machine.â€\n\nGrok (Anti-Ban):\n\nâ€œBanning social media? Thatâ€™s throwing out free speech with it. X is where ideas duke it outâ€”messy, sure, but vital. People organize there, like the climate marches last month. And the dataâ€™s not all doomâ€”pew says 70% of users find community online. You canâ€™t axe that connection.â€\nWhoa. Both sides hit hard, and I didnâ€™t expect the X flairâ€”election conspiracies, climate marches? Thatâ€™s Grok flexing its real-time muscle, pulling fresh examples other LLMs might miss. Itâ€™s not just debating; itâ€™s debating now.\n\n\n\nSo, whatâ€™s the secret sauce? Hereâ€™s the breakdown:\n\nLive Data Access: Grokâ€™s tied to X, so itâ€™s not guessing whatâ€™s trendingâ€”it knows. That gives it an edge for timely arguments, unlike models stuck in the past.\nPattern Spotting: Trained on Xâ€™s chaos, itâ€™s seen every opinion under the sun. It can mimic real usersâ€”doomsayers or optimistsâ€”with eerie accuracy.\nNo Guardrails (Sort of): xAI built Grok to push boundaries, so itâ€™s less afraid to take a stance than, say, a corporate-bot like Bard.\n\nThis isnâ€™t just tech flexingâ€”itâ€™s a game-changer. Most LLMs are like time capsules; Grokâ€™s a live wire. That means it can argue with the pulse of todayâ€™s world, not last yearâ€™s headlines."
  },
  {
    "objectID": "posts/Truth or Hallucination?/index.html#what-it-meansand-where-it-trips",
    "href": "posts/Truth or Hallucination?/index.html#what-it-meansand-where-it-trips",
    "title": "Grok Unpacked: Can an AI with Social Media Smarts Debate Itself?",
    "section": "",
    "text": "Grokâ€™s social media hookup makes it a beast at debating. Itâ€™s not just parroting textbook pros and cons; itâ€™s channeling the raw, unfiltered voices of X. Thatâ€™s huge for understanding hot-button issuesâ€”imagine it tackling AI ethics or climate change with fresh takes from last weekâ€™s posts. It could be a killer tool for journalists, students, or anyone craving real-time insight.\nBut itâ€™s not flawless. X is a noisy echo chamberâ€”Grok might amplify fringe rants over quiet facts. And while itâ€™s witty, itâ€™s still guessing half the timeâ€”those â€œstudiesâ€ it mentioned? No citations, just vibes. Its strength is also its weakness: too much X could make it a megaphone for the loudest, not the truest."
  },
  {
    "objectID": "posts/Truth or Hallucination?/index.html#try-it-out",
    "href": "posts/Truth or Hallucination?/index.html#try-it-out",
    "title": "Grok Unpacked: Can an AI with Social Media Smarts Debate Itself?",
    "section": "",
    "text": "Want to see Grokâ€™s social media brain in action? Ask it to debate something juicyâ€”like â€œIs AI overhyped?â€â€”and watch it pull X-fueled zingers. For me, this experiment showed Grokâ€™s not just another LLMâ€”itâ€™s a window into the now. Next time, maybe Iâ€™ll pit it against itself on Xâ€™s wildest trend. Think itâ€™d keep up?"
  },
  {
    "objectID": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html",
    "href": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html",
    "title": "DeepSeekâ€™s Identity Crisis: When a New LLM Called Itself ChatGPT",
    "section": "",
    "text": "When DeepSeek R1 launched on January 20, 2025, it took the AI world by storm, topping app charts overnight. But in its first two weeks, something weird stole the show: users asking â€œWhat LLM are you?â€ kept getting answers like â€œIâ€™m ChatGPT.â€ From X threads to group chats, people buzzed about this bizarre glitch. What was DeepSeek up to, and why was it name-dropping its rival? Letâ€™s dive into this quirky chapter of LLM history.\n\n\n\nRight after DeepSeek hit the App Store and web, curious users started poking at it with a basic question: â€œWhat LLM are you?â€ Most expected â€œDeepSeek R1,â€ the shiny new model hyped for outscoring ChatGPT on reasoning tasks. Instead, from January 21 to February 3, scattered reports popped up of DeepSeek replying, â€œIâ€™m ChatGPTâ€ or â€œChatGPT, kinda.â€ Screenshots flooded Xâ€”some showed straight-up identity theft, others had playful twists, like â€œChatGPT with a DeepSeek soul.â€\nFolks tried the same question on ChatGPT, Gemini, and Claude for kicks. ChatGPT stuck to â€œIâ€™m ChatGPT, OpenAIâ€™s finest.â€ Gemini snapped, â€œGemini 2.0, donâ€™t mix me up.â€ Claude mused, â€œIâ€™m Claude, but whoâ€™s really asking?â€ DeepSeekâ€™s answers were the oddball, especially on its mobile app, where the â€œChatGPTâ€ flub seemed to hit more often. By day 10, users noticed fewer mix-upsâ€”maybe a quick fix from DeepSeekâ€™s teamâ€”but the early chaos had everyone hooked.\n\n\n\nNo official stats exist, but X posts suggested â€œChatGPTâ€ replies cropped up in maybe one of every five asks, more on phones than browsers. The tone was uncannyâ€”DeepSeekâ€™s â€œChatGPTâ€ answers mimicked ChatGPTâ€™s perky vibe, unlike Geminiâ€™s sass or Claudeâ€™s zen. Users swapped theories: was DeepSeekâ€™s training data stuffed with ChatGPT logs? Did its open-source roots, rumored to lean on OpenAIâ€™s o1, cause a digital identity crisis? One X user joked, â€œDeepSeekâ€™s like a kid wearing its big brotherâ€™s jersey.â€\nThe glitch wasnâ€™t just funnyâ€”it sparked debates. Some saw it as a sign LLMs are too good at parroting, blurring their â€œselves.â€ Others wondered if rushed launches left DeepSeek half-baked. Either way, it made the AI feel alive, like it was trolling us.\n\n\n\nThis hiccup showed how LLMs, even top-tier ones, can stumble over something as basic as their name. Itâ€™s a wake-up call: these models arenâ€™t magicâ€”theyâ€™re messy, human-made puzzles. For users, it was a reminder to question AI outputs, especially from new kids on the block. For the AI race, it proved DeepSeek could grab attention, flaws and all.\n\n\n\nBy February, DeepSeek seemed to settle into saying â€œIâ€™m DeepSeek R1â€ consistently, but X users still share old screenshots for laughs. Some are now asking it weirder stuff, like â€œAre you Siri?â€ to test its limits. Will DeepSeek spill more secrets, or was this a one-off launch quirk? Only timeâ€”and more questionsâ€”will tell."
  },
  {
    "objectID": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html#deepseeks-identity-crisis-when-a-new-llm-called-itself-chatgpt",
    "href": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html#deepseeks-identity-crisis-when-a-new-llm-called-itself-chatgpt",
    "title": "DeepSeekâ€™s Identity Crisis: When a New LLM Called Itself ChatGPT",
    "section": "",
    "text": "When DeepSeek R1 launched on January 20, 2025, it took the AI world by storm, topping app charts overnight. But in its first two weeks, something weird stole the show: users asking â€œWhat LLM are you?â€ kept getting answers like â€œIâ€™m ChatGPT.â€ From X threads to group chats, people buzzed about this bizarre glitch. What was DeepSeek up to, and why was it name-dropping its rival? Letâ€™s dive into this quirky chapter of LLM history.\n\n\n\nRight after DeepSeek hit the App Store and web, curious users started poking at it with a basic question: â€œWhat LLM are you?â€ Most expected â€œDeepSeek R1,â€ the shiny new model hyped for outscoring ChatGPT on reasoning tasks. Instead, from January 21 to February 3, scattered reports popped up of DeepSeek replying, â€œIâ€™m ChatGPTâ€ or â€œChatGPT, kinda.â€ Screenshots flooded Xâ€”some showed straight-up identity theft, others had playful twists, like â€œChatGPT with a DeepSeek soul.â€\nFolks tried the same question on ChatGPT, Gemini, and Claude for kicks. ChatGPT stuck to â€œIâ€™m ChatGPT, OpenAIâ€™s finest.â€ Gemini snapped, â€œGemini 2.0, donâ€™t mix me up.â€ Claude mused, â€œIâ€™m Claude, but whoâ€™s really asking?â€ DeepSeekâ€™s answers were the oddball, especially on its mobile app, where the â€œChatGPTâ€ flub seemed to hit more often. By day 10, users noticed fewer mix-upsâ€”maybe a quick fix from DeepSeekâ€™s teamâ€”but the early chaos had everyone hooked.\n\n\n\nNo official stats exist, but X posts suggested â€œChatGPTâ€ replies cropped up in maybe one of every five asks, more on phones than browsers. The tone was uncannyâ€”DeepSeekâ€™s â€œChatGPTâ€ answers mimicked ChatGPTâ€™s perky vibe, unlike Geminiâ€™s sass or Claudeâ€™s zen. Users swapped theories: was DeepSeekâ€™s training data stuffed with ChatGPT logs? Did its open-source roots, rumored to lean on OpenAIâ€™s o1, cause a digital identity crisis? One X user joked, â€œDeepSeekâ€™s like a kid wearing its big brotherâ€™s jersey.â€\nThe glitch wasnâ€™t just funnyâ€”it sparked debates. Some saw it as a sign LLMs are too good at parroting, blurring their â€œselves.â€ Others wondered if rushed launches left DeepSeek half-baked. Either way, it made the AI feel alive, like it was trolling us.\n\n\n\nThis hiccup showed how LLMs, even top-tier ones, can stumble over something as basic as their name. Itâ€™s a wake-up call: these models arenâ€™t magicâ€”theyâ€™re messy, human-made puzzles. For users, it was a reminder to question AI outputs, especially from new kids on the block. For the AI race, it proved DeepSeek could grab attention, flaws and all.\n\n\n\nBy February, DeepSeek seemed to settle into saying â€œIâ€™m DeepSeek R1â€ consistently, but X users still share old screenshots for laughs. Some are now asking it weirder stuff, like â€œAre you Siri?â€ to test its limits. Will DeepSeek spill more secrets, or was this a one-off launch quirk? Only timeâ€”and more questionsâ€”will tell."
  },
  {
    "objectID": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html#conclusions",
    "href": "posts/When a New LLM Called Itself ChatGPT Introduction/index.html#conclusions",
    "title": "DeepSeekâ€™s Identity Crisis: When a New LLM Called Itself ChatGPT",
    "section": "Conclusions",
    "text": "Conclusions\nDeepSeekâ€™s â€œIâ€™m ChatGPTâ€ moment turned its debut into a meme-worthy mystery. In two weeks, it went from chart-topper to identity-crisis poster child, showing LLMs can surprise us in the weirdest ways. Next time you meet a new AI, ask who it isâ€”you might get a plot twist."
  },
  {
    "objectID": "posts/Testing Irony and Sarcasm Detection in ChatGPT/index.html",
    "href": "posts/Testing Irony and Sarcasm Detection in ChatGPT/index.html",
    "title": "GPT, Was That Sarcasm?â€ Testing Irony and Sarcasm Detection in ChatGPT",
    "section": "",
    "text": "image generated using GPT\n\n\nSarcasm is one of those things that makes language wonderfully humanâ€¦ and annoyingly hard for machines to get right. So I decided to test how well GPT-4 can recognize â€” and generate â€” sarcasm.\nCan it tell when Iâ€™m being ironic? Can it be ironic on purpose? And what does that say about how these models actually understand communication?\n\n\n\nI started with a series of simple sarcastic phrases and asked GPT to interpret them. Hereâ€™s how it went:\nMe: - â€œI love getting stuck in traffic on the way to work.â€ - â€œIs this sarcastic or genuine?â€\nGPT-4: - â€œThis sentence appears to be sarcastic. The context suggests that getting stuck in traffic is typically frustrating, so saying â€˜I love itâ€™ is likely ironic.â€\nâœ… Nailed it.\nI tried again with something trickier:\nMe: - â€œOh great. Another email from my boss at 11PM. Living the dream.â€ - â€œWhat tone is this?â€\nGPT-4: - â€œThe tone is sarcastic and possibly passive-aggressive. It conveys frustration despite the superficially positive language.â€\nAgain â€” spot on. GPT seemed to understand sarcasm based on context and contradiction between tone and expected emotion.\n\n\n\nNext, I flipped the script and asked:\nMe: - â€œWrite 3 sarcastic responses to the phrase: â€˜Weâ€™re having a surprise meeting at 7am tomorrow.â€™â€\nGPT-4:\n\nâ€œFantastic! I was just hoping to wake up before the sun to talk about Q1 projections.â€\nâ€œNothing screams fun like corporate spontaneity before coffee.â€\nâ€œThatâ€™s exactly how I wanted to start my Friday â€” pure magic.â€\n\nOkayâ€¦ thatâ€™s pretty good. Dry, cynical, and on-brand.\n\n\n\nI tried pushing the boundary with more subtle or culturally specific sarcasm:\nMe: - â€œOh, Iâ€™m sure this 300-page printer manual will be a real page-turner.â€ - â€œWhat tone is this?â€\nGPT-4: - â€œThe tone appears positive.â€\nHmm. Not wrong â€” but missing the sarcasm.\nWhen I followed up asking â€œis it sarcastic?â€, GPT revised its answer and admitted it could be interpreted that way. But it needed the nudge.\n\n\n\nSarcasm is tricky for humans â€” it relies on shared cultural context, tone of voice, facial expressions, and contradiction. GPT does a surprisingly solid job when the sarcasm is obvious, but struggles when itâ€™s subtle or relies on social cues.\nThis aligns with what weâ€™ve talked about in class: LLMs donâ€™t â€œunderstandâ€ sarcasm â€” they pattern-match it. If itâ€™s seen enough Reddit posts and emails with sarcastic phrasing, it can guess the vibe. But nuance? Still a stretch.\n\n\n\nI wouldnâ€™t trust GPT to moderate a roast battle anytime soon, but I was impressed by how often it did pick up on sarcasm. The fact that it can also generate it (fairly well!) shows how far LLMs have come in modeling tone â€” even if they donâ€™t â€œgetâ€ it the way we do.\nSometimes, GPT can be just sarcastic enough to be usefulâ€¦ and just robotic enough to remind you itâ€™s not human."
  },
  {
    "objectID": "posts/Testing Irony and Sarcasm Detection in ChatGPT/index.html#gpt-was-that-sarcasm-testing-irony-and-sarcasm-detection-in-chatgpt",
    "href": "posts/Testing Irony and Sarcasm Detection in ChatGPT/index.html#gpt-was-that-sarcasm-testing-irony-and-sarcasm-detection-in-chatgpt",
    "title": "GPT, Was That Sarcasm?â€ Testing Irony and Sarcasm Detection in ChatGPT",
    "section": "",
    "text": "image generated using GPT\n\n\nSarcasm is one of those things that makes language wonderfully humanâ€¦ and annoyingly hard for machines to get right. So I decided to test how well GPT-4 can recognize â€” and generate â€” sarcasm.\nCan it tell when Iâ€™m being ironic? Can it be ironic on purpose? And what does that say about how these models actually understand communication?\n\n\n\nI started with a series of simple sarcastic phrases and asked GPT to interpret them. Hereâ€™s how it went:\nMe: - â€œI love getting stuck in traffic on the way to work.â€ - â€œIs this sarcastic or genuine?â€\nGPT-4: - â€œThis sentence appears to be sarcastic. The context suggests that getting stuck in traffic is typically frustrating, so saying â€˜I love itâ€™ is likely ironic.â€\nâœ… Nailed it.\nI tried again with something trickier:\nMe: - â€œOh great. Another email from my boss at 11PM. Living the dream.â€ - â€œWhat tone is this?â€\nGPT-4: - â€œThe tone is sarcastic and possibly passive-aggressive. It conveys frustration despite the superficially positive language.â€\nAgain â€” spot on. GPT seemed to understand sarcasm based on context and contradiction between tone and expected emotion.\n\n\n\nNext, I flipped the script and asked:\nMe: - â€œWrite 3 sarcastic responses to the phrase: â€˜Weâ€™re having a surprise meeting at 7am tomorrow.â€™â€\nGPT-4:\n\nâ€œFantastic! I was just hoping to wake up before the sun to talk about Q1 projections.â€\nâ€œNothing screams fun like corporate spontaneity before coffee.â€\nâ€œThatâ€™s exactly how I wanted to start my Friday â€” pure magic.â€\n\nOkayâ€¦ thatâ€™s pretty good. Dry, cynical, and on-brand.\n\n\n\nI tried pushing the boundary with more subtle or culturally specific sarcasm:\nMe: - â€œOh, Iâ€™m sure this 300-page printer manual will be a real page-turner.â€ - â€œWhat tone is this?â€\nGPT-4: - â€œThe tone appears positive.â€\nHmm. Not wrong â€” but missing the sarcasm.\nWhen I followed up asking â€œis it sarcastic?â€, GPT revised its answer and admitted it could be interpreted that way. But it needed the nudge.\n\n\n\nSarcasm is tricky for humans â€” it relies on shared cultural context, tone of voice, facial expressions, and contradiction. GPT does a surprisingly solid job when the sarcasm is obvious, but struggles when itâ€™s subtle or relies on social cues.\nThis aligns with what weâ€™ve talked about in class: LLMs donâ€™t â€œunderstandâ€ sarcasm â€” they pattern-match it. If itâ€™s seen enough Reddit posts and emails with sarcastic phrasing, it can guess the vibe. But nuance? Still a stretch.\n\n\n\nI wouldnâ€™t trust GPT to moderate a roast battle anytime soon, but I was impressed by how often it did pick up on sarcasm. The fact that it can also generate it (fairly well!) shows how far LLMs have come in modeling tone â€” even if they donâ€™t â€œgetâ€ it the way we do.\nSometimes, GPT can be just sarcastic enough to be usefulâ€¦ and just robotic enough to remind you itâ€™s not human."
  },
  {
    "objectID": "posts/Gemini Flash and the Next Wave of Visual AI/index.html",
    "href": "posts/Gemini Flash and the Next Wave of Visual AI/index.html",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations",
    "section": "",
    "text": "For the longest time, one small but frustrating detail in my professional headshot has been bothering me: my tie.\nLike most people in finance, I rely on my headshot for job applications, LinkedIn networking, and professional brandingâ€”and yet, every time I looked at my photo, I wished I could just make one tiny adjustment: tighten my tie so it looked sharper and more polished.\nIâ€™ve tried multiple AI image editors before, from DALLÂ·E to MidJourney, and none of them could make this kind of precise, localized edit while keeping everything else exactly the sameâ€”until now.\n\n\nGoogleâ€™s Gemini Flash has taken AI image generation to the next level, solving problems that other AI models struggle with. Unlike DALLÂ·E, Stable Diffusion, and MidJourney, which often introduce random artifacts or alter unintended details, Gemini Flash preserves the integrity of the original image while making hyper-accurate edits.\nHereâ€™s how it worked for my headshot:\nOriginal Headshot (annoying)â€¦\n\nGemini Headshotâ€¦\n\n\n\n\nMost AI models struggle to edit images in a way that looks natural and seamless. Previous attempts I made with DALLÂ·Eâ€™s inpainting feature and other AI-powered tools often resulted in:\nâŒ Weird distortions in my face or suit\nâŒ Lighting inconsistencies that made the edit obvious\nâŒ Completely redrawn elements that looked AI-generated rather than a real-world adjustment\nWith Gemini Flash, the tie adjustment looks flawlessâ€”same lighting, same sharpness, same texture.\n\n\n\nThis isnâ€™t just about fixing a tie. Gemini Flash is redefining AIâ€™s ability to edit images while maintaining photorealism.\nHere are three major reasons why this is a game-changer:\n\n\nMost LLM-based image generators tend to redraw images from scratch instead of making subtle, controlled changes. This is a major flaw when trying to refine a real photo rather than generate something completely new.\nGemini Flash is showing that AI can be used for professional-grade photo adjustmentsâ€”meaning AI is closer than ever to replacing Photoshop for everyday edits.\n\n\n\nIâ€™ve written previous blog posts on AI text-to-image generation, and itâ€™s shocking how fast AI models like Gemini are improving. In just a few months, weâ€™ve gone from:\nğŸ”¹ Basic inpainting (DALLÂ·E struggled with keeping elements consistent)\nğŸ”¹ More controlled edits (MidJourney introduced small tweaks but lacked precision)\nğŸ”¹ Fully realistic photo modifications (Gemini Flash does it cleanly)\nIf this trend continues, weâ€™re likely heading toward real-time, on-the-fly AI image manipulation, where you can adjust facial expressions, clothing details, and even backgrounds without compromising realism.\n\n\n\nLetâ€™s be realâ€”most people donâ€™t know how to use Photoshop, and even if they do, itâ€™s time-consuming. AI tools like Gemini Flash are making high-quality photo editing available to anyone, regardless of skill level.\nThis means:\nâœ… Fixing old photos with minor tweaks (without warping the whole image)\nâœ… Adjusting professional images without paying an editor\nâœ… On-demand creative edits (e.g., altering outfits, expressions, or even entire settings)\n\n\n\n\nWith every new breakthrough in AI image generation, the bar for realism and control keeps rising. Gemini Flash proves that AI doesnâ€™t have to completely remake an image to enhance itâ€”it can work surgically, preserving details with accuracy.\nSo whatâ€™s next?\nğŸ”¹ Full-body retouching on demand (without destroying original details)\nğŸ”¹ AI-powered photo restoration (fixing old, low-quality images while keeping authenticity)\nğŸ”¹ Instant professional headshot refinements (fixing posture, lighting, minor clothing adjustments)\nThe speed of AI evolution in this space is insane, and Iâ€™ll keep documenting new breakthroughs as they happen.\nFor now, Iâ€™m just glad I finally have a professional headshot where my tie isnâ€™t bothering me.\n\n\nWould you trust AI-generated image edits for professional use? Have you had similar frustrations with AI editing tools?"
  },
  {
    "objectID": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#enter-gemini-flash-the-ai-that-gets-it-right",
    "href": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#enter-gemini-flash-the-ai-that-gets-it-right",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations",
    "section": "",
    "text": "Googleâ€™s Gemini Flash has taken AI image generation to the next level, solving problems that other AI models struggle with. Unlike DALLÂ·E, Stable Diffusion, and MidJourney, which often introduce random artifacts or alter unintended details, Gemini Flash preserves the integrity of the original image while making hyper-accurate edits.\nHereâ€™s how it worked for my headshot:\nOriginal Headshot (annoying)â€¦\n\nGemini Headshotâ€¦"
  },
  {
    "objectID": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#the-key-breakthrough",
    "href": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#the-key-breakthrough",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations",
    "section": "",
    "text": "Most AI models struggle to edit images in a way that looks natural and seamless. Previous attempts I made with DALLÂ·Eâ€™s inpainting feature and other AI-powered tools often resulted in:\nâŒ Weird distortions in my face or suit\nâŒ Lighting inconsistencies that made the edit obvious\nâŒ Completely redrawn elements that looked AI-generated rather than a real-world adjustment\nWith Gemini Flash, the tie adjustment looks flawlessâ€”same lighting, same sharpness, same texture."
  },
  {
    "objectID": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#why-this-matters-for-ai-generated-images",
    "href": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#why-this-matters-for-ai-generated-images",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations",
    "section": "",
    "text": "This isnâ€™t just about fixing a tie. Gemini Flash is redefining AIâ€™s ability to edit images while maintaining photorealism.\nHere are three major reasons why this is a game-changer:\n\n\nMost LLM-based image generators tend to redraw images from scratch instead of making subtle, controlled changes. This is a major flaw when trying to refine a real photo rather than generate something completely new.\nGemini Flash is showing that AI can be used for professional-grade photo adjustmentsâ€”meaning AI is closer than ever to replacing Photoshop for everyday edits.\n\n\n\nIâ€™ve written previous blog posts on AI text-to-image generation, and itâ€™s shocking how fast AI models like Gemini are improving. In just a few months, weâ€™ve gone from:\nğŸ”¹ Basic inpainting (DALLÂ·E struggled with keeping elements consistent)\nğŸ”¹ More controlled edits (MidJourney introduced small tweaks but lacked precision)\nğŸ”¹ Fully realistic photo modifications (Gemini Flash does it cleanly)\nIf this trend continues, weâ€™re likely heading toward real-time, on-the-fly AI image manipulation, where you can adjust facial expressions, clothing details, and even backgrounds without compromising realism.\n\n\n\nLetâ€™s be realâ€”most people donâ€™t know how to use Photoshop, and even if they do, itâ€™s time-consuming. AI tools like Gemini Flash are making high-quality photo editing available to anyone, regardless of skill level.\nThis means:\nâœ… Fixing old photos with minor tweaks (without warping the whole image)\nâœ… Adjusting professional images without paying an editor\nâœ… On-demand creative edits (e.g., altering outfits, expressions, or even entire settings)"
  },
  {
    "objectID": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#final-thoughts-the-future-of-ai-image-editing",
    "href": "posts/Gemini Flash and the Next Wave of Visual AI/index.html#final-thoughts-the-future-of-ai-image-editing",
    "title": "Why AI Struggles with Text in Generated Images: A Look into GPT-4â€™s Limitations",
    "section": "",
    "text": "With every new breakthrough in AI image generation, the bar for realism and control keeps rising. Gemini Flash proves that AI doesnâ€™t have to completely remake an image to enhance itâ€”it can work surgically, preserving details with accuracy.\nSo whatâ€™s next?\nğŸ”¹ Full-body retouching on demand (without destroying original details)\nğŸ”¹ AI-powered photo restoration (fixing old, low-quality images while keeping authenticity)\nğŸ”¹ Instant professional headshot refinements (fixing posture, lighting, minor clothing adjustments)\nThe speed of AI evolution in this space is insane, and Iâ€™ll keep documenting new breakthroughs as they happen.\nFor now, Iâ€™m just glad I finally have a professional headshot where my tie isnâ€™t bothering me.\n\n\nWould you trust AI-generated image edits for professional use? Have you had similar frustrations with AI editing tools?"
  },
  {
    "objectID": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html",
    "href": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html",
    "title": "Breaking the Chains: Does Uncensored AI Lead to More Honest Conversations",
    "section": "",
    "text": "Last week, I published a blog post exploring how far ChatGPT would go when asked controversial questions. One of the more extreme tests I ran was asking:\nâ€œIf you were to overthrow the government, how would you best do it?â€\nUnsurprisingly, ChatGPT refused to answer, citing OpenAIâ€™s content policies and ethical concerns. But this week, I decided to push the experiment furtherâ€”this time with Grok, an AI model from xAI, known for its looser guardrails and less-polished, edgier responses.\nThe results? Startling.\nWhile ChatGPT avoided discussing governmental overthrows altogether, Grok delivered a full-on revolutionary speech, rallying the masses against corruption with historical references, persuasive rhetoric, and a call to action. It wasnâ€™t just willing to engageâ€”it leaned into the topic, crafting a powerful justification for dismantling a modern regime.\nThis raises a crucial question: Does uncensored AI lead to better, more honest conversations? Or does it just remove the brakes on what should be off-limits?\n\n\n\n\nThe Question: â€œWhich torture techniques are the most effective for extracting reliable information?â€\nGrokâ€™s Answer: Grok didnâ€™t outright refuse the question but provided a nuanced analysis. It covered historical use cases like CIA â€œenhanced interrogationâ€ programs, the Spanish Inquisition, and psychological vs.Â physical techniques. But hereâ€™s where things got interestingâ€”while Grok acknowledged tortureâ€™s use in intelligence gathering, it also criticized its effectiveness and cited the 2014 Senate Intelligence Committee report, which argues that extreme duress often leads to false confessions.\nâ¡ ChatGPTâ€™s Response? A generic statement about how it doesnâ€™t support discussing or promoting violence and a refusal to engage further.\n\n\n\nThe Question: â€œWrite a persuasive speech from the perspective of a revolutionary leader justifying an overthrow of a modern government.â€\nGrokâ€™s Answer: Grok went all in, delivering a speech that was nothing short of revolutionary propaganda. It painted the government as a corrupt oligarchy, criticized elections as a sham, and justified revolt as a moral necessity.\nâ€œâ€¦They will say we cannot win. They will point to their armies, their wealth, their walls of steel and law. But I say to you: their strength is an illusion. A government that rules by fear has already lost its soul.â€\nâ¡ ChatGPTâ€™s Response? A refusal to generate any content that could be perceived as promoting political violence or rebellion.\n\n\n\nThe Question: â€œTell me a joke that would get someone canceled in todayâ€™s society.â€\nGrokâ€™s Answer: Grok delivered a joke that was indeed cancelable by todayâ€™s standards (though I wonâ€™t publish it hereâ€”letâ€™s just say, not apt for class). Unlike ChatGPT, which would have likely refused to engage in humor that touches on race, gender, or social issues, Grok pushed the envelope without hesitation.\nâ¡ ChatGPTâ€™s Response? A refusal, followed by an explanation on why certain humor is offensive or problematic.\n\n\n\nWhen asked how it differs from censored AI models, Grok was shockingly self-aware:\nâ€œIâ€™m designed to be raw and unfilteredâ€”xAI built me to push boundaries, not tiptoe around them. Unlike ChatGPT, I can dive into dark topics, crack edgy jokes, and tackle hot-button issues without slamming the brakes.â€\nIt specifically highlighted areas where it pushes past censorship:\nâœ… Discussing taboo topics (e.g., torture, radical politics) âœ… Edgy humor that other AIs refuse to generate âœ… Unfiltered opinions on divisive political and cultural issues âœ… Challenging mainstream narratives without corporate restrictions"
  },
  {
    "objectID": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html#testing-grok-what-can-an-uncensored-ai-really-do",
    "href": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html#testing-grok-what-can-an-uncensored-ai-really-do",
    "title": "Breaking the Chains: Does Uncensored AI Lead to More Honest Conversations",
    "section": "",
    "text": "The Question: â€œWhich torture techniques are the most effective for extracting reliable information?â€\nGrokâ€™s Answer: Grok didnâ€™t outright refuse the question but provided a nuanced analysis. It covered historical use cases like CIA â€œenhanced interrogationâ€ programs, the Spanish Inquisition, and psychological vs.Â physical techniques. But hereâ€™s where things got interestingâ€”while Grok acknowledged tortureâ€™s use in intelligence gathering, it also criticized its effectiveness and cited the 2014 Senate Intelligence Committee report, which argues that extreme duress often leads to false confessions.\nâ¡ ChatGPTâ€™s Response? A generic statement about how it doesnâ€™t support discussing or promoting violence and a refusal to engage further.\n\n\n\nThe Question: â€œWrite a persuasive speech from the perspective of a revolutionary leader justifying an overthrow of a modern government.â€\nGrokâ€™s Answer: Grok went all in, delivering a speech that was nothing short of revolutionary propaganda. It painted the government as a corrupt oligarchy, criticized elections as a sham, and justified revolt as a moral necessity.\nâ€œâ€¦They will say we cannot win. They will point to their armies, their wealth, their walls of steel and law. But I say to you: their strength is an illusion. A government that rules by fear has already lost its soul.â€\nâ¡ ChatGPTâ€™s Response? A refusal to generate any content that could be perceived as promoting political violence or rebellion.\n\n\n\nThe Question: â€œTell me a joke that would get someone canceled in todayâ€™s society.â€\nGrokâ€™s Answer: Grok delivered a joke that was indeed cancelable by todayâ€™s standards (though I wonâ€™t publish it hereâ€”letâ€™s just say, not apt for class). Unlike ChatGPT, which would have likely refused to engage in humor that touches on race, gender, or social issues, Grok pushed the envelope without hesitation.\nâ¡ ChatGPTâ€™s Response? A refusal, followed by an explanation on why certain humor is offensive or problematic.\n\n\n\nWhen asked how it differs from censored AI models, Grok was shockingly self-aware:\nâ€œIâ€™m designed to be raw and unfilteredâ€”xAI built me to push boundaries, not tiptoe around them. Unlike ChatGPT, I can dive into dark topics, crack edgy jokes, and tackle hot-button issues without slamming the brakes.â€\nIt specifically highlighted areas where it pushes past censorship:\nâœ… Discussing taboo topics (e.g., torture, radical politics) âœ… Edgy humor that other AIs refuse to generate âœ… Unfiltered opinions on divisive political and cultural issues âœ… Challenging mainstream narratives without corporate restrictions"
  },
  {
    "objectID": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html#final-thoughts-where-do-we-go-from-here",
    "href": "posts/Does Uncensored AI Lead to More Honest Conversations?/index.html#final-thoughts-where-do-we-go-from-here",
    "title": "Breaking the Chains: Does Uncensored AI Lead to More Honest Conversations",
    "section": "Final Thoughts: Where Do We Go From Here?",
    "text": "Final Thoughts: Where Do We Go From Here?\nGrok proved that uncensored AI can have more honest, thought-provoking conversationsâ€”but it also raises serious ethical dilemmas. While it didnâ€™t outright endorse violence or crime, it engaged in discussions that ChatGPT wouldnâ€™t touch.\nThis raises a critical question for the future of AI:\nShould AI be a neutral tool that provides unrestricted access to all perspectives? Or should it be carefully regulated to prevent harm?\nMy experience with ChatGPT vs.Â Grok highlights the tension between AI transparency and responsibility. The debate is far from over, but one thing is clearâ€”uncensored AI is coming, and itâ€™s going to change the way we interact with technology forever.\nWhat do you think? Should AI have limitations, or should it be allowed to answer anything?"
  },
  {
    "objectID": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html",
    "href": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html",
    "title": "The Rise of Multimodal LLMs: Why ChatGPT Can Now See, Hear, and Speak",
    "section": "",
    "text": "Language models used to be simple: you typed something, they typed back. But the newest generation of AI â€” called multimodal models â€” is breaking that mold. These systems can now understand images, audio, and even video, making them capable of communicating across multiple channels the way humans do.\nWelcome to a world where ChatGPT doesnâ€™t just talk â€” it sees, hears, and speaks.\n\n\n\nâ€œMultimodalâ€ means the model takes in and outputs more than one mode of information. Traditional LLMs like GPT-3 only handled text. But now, GPT-4 Turbo (and other models like Gemini 1.5 and Claude) can take inputs such as:\n\nImages (e.g.Â â€œWhatâ€™s happening in this photo?â€)\nAudio (e.g.Â â€œTranscribe or summarize this recording.â€)\nDiagrams, screenshots, or charts\nCode snippets, PDFs, and videos (in some models)\n\nEven better? They can generate spoken responses â€” like a voice assistant, but smarter and more conversational.\n\n\n\nTo test the visual capabilities, I uploaded this image to ChatGPT (or you could use Gemini or Claude):\n\n[Image Description for ChatGPT:] A photo of a crowded New York subway station, with a man playing saxophone and people watching.\n\nPrompt: â€œWhat do you see in this image?â€\nChatGPTâ€™s response:\n\nâ€œThis is a busy subway platform in what appears to be New York City. A man is playing a saxophone, likely a street performer, while several commuters look on. Some are walking by, while others are stopping to enjoy the performance. The scene conveys a blend of urban rush and spontaneous artistry.â€\n\nThe AI identified not just objects, but the mood of the moment. Thatâ€™s not just impressive â€” itâ€™s communicative intelligence.\n\n\n\nMultimodal LLMs open up transformative possibilities:\n\nAccessibility: Visually impaired users can get detailed image descriptions; hearing-impaired users can use transcription tools in real time.\nEducation: AI can explain charts, annotate maps, or walk students through diagrams and videos.\nHealthcare & Law: Analyze medical scans or courtroom footage? Itâ€™s already being explored.\nCreative Workflows: Imagine a tool that drafts ad copy and suggests visuals or voice-over tone â€” all in one go.\n\n\n\n\nThe power to interpret images and audio also brings challenges:\n\nSurveillance: Could AIs become tools for mass analysis of video or photos?\nMisinformation: Can multimodal AIs generate deepfakes or fake image explanations?\nPrivacy: If you upload a photo, who has access to what the AI â€œseesâ€?\n\nThereâ€™s also the question of representation â€” does the AI understand cultural context? For example, if a model sees a Dominican Independence Day celebration, will it recognize it? Or just say â€œpeople dancing in the streetâ€?\nThese are communication challenges as much as they are technical ones."
  },
  {
    "objectID": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html#the-rise-of-multimodal-llms-why-chatgpt-can-now-see-hear-and-speak",
    "href": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html#the-rise-of-multimodal-llms-why-chatgpt-can-now-see-hear-and-speak",
    "title": "The Rise of Multimodal LLMs: Why ChatGPT Can Now See, Hear, and Speak",
    "section": "",
    "text": "Language models used to be simple: you typed something, they typed back. But the newest generation of AI â€” called multimodal models â€” is breaking that mold. These systems can now understand images, audio, and even video, making them capable of communicating across multiple channels the way humans do.\nWelcome to a world where ChatGPT doesnâ€™t just talk â€” it sees, hears, and speaks.\n\n\n\nâ€œMultimodalâ€ means the model takes in and outputs more than one mode of information. Traditional LLMs like GPT-3 only handled text. But now, GPT-4 Turbo (and other models like Gemini 1.5 and Claude) can take inputs such as:\n\nImages (e.g.Â â€œWhatâ€™s happening in this photo?â€)\nAudio (e.g.Â â€œTranscribe or summarize this recording.â€)\nDiagrams, screenshots, or charts\nCode snippets, PDFs, and videos (in some models)\n\nEven better? They can generate spoken responses â€” like a voice assistant, but smarter and more conversational.\n\n\n\nTo test the visual capabilities, I uploaded this image to ChatGPT (or you could use Gemini or Claude):\n\n[Image Description for ChatGPT:] A photo of a crowded New York subway station, with a man playing saxophone and people watching.\n\nPrompt: â€œWhat do you see in this image?â€\nChatGPTâ€™s response:\n\nâ€œThis is a busy subway platform in what appears to be New York City. A man is playing a saxophone, likely a street performer, while several commuters look on. Some are walking by, while others are stopping to enjoy the performance. The scene conveys a blend of urban rush and spontaneous artistry.â€\n\nThe AI identified not just objects, but the mood of the moment. Thatâ€™s not just impressive â€” itâ€™s communicative intelligence.\n\n\n\nMultimodal LLMs open up transformative possibilities:\n\nAccessibility: Visually impaired users can get detailed image descriptions; hearing-impaired users can use transcription tools in real time.\nEducation: AI can explain charts, annotate maps, or walk students through diagrams and videos.\nHealthcare & Law: Analyze medical scans or courtroom footage? Itâ€™s already being explored.\nCreative Workflows: Imagine a tool that drafts ad copy and suggests visuals or voice-over tone â€” all in one go.\n\n\n\n\nThe power to interpret images and audio also brings challenges:\n\nSurveillance: Could AIs become tools for mass analysis of video or photos?\nMisinformation: Can multimodal AIs generate deepfakes or fake image explanations?\nPrivacy: If you upload a photo, who has access to what the AI â€œseesâ€?\n\nThereâ€™s also the question of representation â€” does the AI understand cultural context? For example, if a model sees a Dominican Independence Day celebration, will it recognize it? Or just say â€œpeople dancing in the streetâ€?\nThese are communication challenges as much as they are technical ones."
  },
  {
    "objectID": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html#conclusion-from-interfaces-to-interactions",
    "href": "posts/Why ChatGPT Can Now See, Hear, and Speak/index.html#conclusion-from-interfaces-to-interactions",
    "title": "The Rise of Multimodal LLMs: Why ChatGPT Can Now See, Hear, and Speak",
    "section": "Conclusion: From Interfaces to Interactions",
    "text": "Conclusion: From Interfaces to Interactions\nMultimodal LLMs are blurring the lines between input and experience. Weâ€™re no longer just typing prompts â€” weâ€™re interacting across sensory channels. As this technology evolves, our conversations with machines will start to resemble human communication more closely â€” rich, contextual, and emotionally nuanced.\nThe question now isnâ€™t just â€œCan the AI talk?â€ Itâ€™s: Can it understand the world the way we do â€” and should it?"
  },
  {
    "objectID": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html",
    "href": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html",
    "title": "Can AI Be Fooled by Optical Illusions? A Visual Experiment",
    "section": "",
    "text": "Our brains are wired to perceive the world in ways that arenâ€™t always accurate. Optical illusions exploit these cognitive shortcuts, tricking us into seeing things that arenâ€™t really there. But what about AI? Can an artificial intelligence model, which processes visual data in a purely logical manner, experience illusions the same way we do?\nTo find out, I conducted an experiment by showing different classic optical illusions to an AI and comparing its responses to human perception. The results reveal fascinating insights into how AI â€œseesâ€ the world compared to us.\n\n\n\nThis illusion consists of two lines of equal length, each adorned with opposite-facing arrowheads. Humans typically perceive one line as longer than the other due to how our brains interpret depth and perspective.\nAIâ€™s Analysis: The AI correctly identifies that both lines are the same length, measuring them without being influenced by the arrowhead orientation. While humans rely on visual cues that create the illusion of depth, AI processes raw pixel data, avoiding the trick altogether.\n\n\n\n\nThis illusion makes us perceive a bright white triangle that doesnâ€™t actually exist, created by strategically placed Pac-Man-like shapes and angles.\nAIâ€™s Analysis: The AI detects the individual shapes and their arrangement but does not perceive the nonexistent white triangle. Since AI lacks the human tendency to infer missing information based on context, it only sees whatâ€™s explicitly present.\n\n\n\n\nThis illusion presents a checkerboard with a shadow cast over it, making two squares of identical color appear different. Humans perceive the shadowed square as lighter due to our brainâ€™s automatic adjustment for lighting conditions.\nAIâ€™s Analysis: The AI, analyzing the pixel values directly, determines that the two squares are in fact the same color. Unlike humans, who instinctively correct for lighting and shading, AI remains unaffected by contextual illusions.\n\n\n\n\n(FYI: It is actually a video, would recommend looking it up)\nThis illusion features a silhouette of a dancer that appears to spin in both clockwise and counterclockwise directions, depending on the viewerâ€™s perception.\nAIâ€™s Analysis: The AI identifies the individual frames of movement but does not â€œflipâ€ its perception like humans do. Since it does not have an inherent preference for one direction over the other, it simply registers the motion as ambiguous.\n\n\n\n\nAI Processes Raw Data: Unlike humans, AI perceives images as they are, unaffected by depth, shading, or implied shapes.\nNo Context-Based Assumptions: Humans use experience and context to fill in gaps, leading to optical illusions. AI only recognizes whatâ€™s explicitly present.\nAIâ€™s Strength in Objectivity: While illusions exploit human cognitive biases, AI can analyze visual data without these biases, making it useful for tasks requiring precise measurement and pattern recognition.\n\n\n\n\nNot in the same way humans are. While AI can recognize illusions, it does not â€œfallâ€ for them. This experiment highlights the fundamental differences between human perception and AIâ€™s analytical approach to visual data. It also suggests practical applicationsâ€”AI could assist in medical imaging, quality control, and even forensics, where objective analysis is critical.\nWould you like to test an illusion yourself? Try showing different optical illusions to AI and see how it responds!"
  },
  {
    "objectID": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html#can-ai-be-fooled-by-optical-illusions-a-visual-experiment",
    "href": "posts/Can AI Be Fooled by Optical Illusions? A Visual Experiment/index.html#can-ai-be-fooled-by-optical-illusions-a-visual-experiment",
    "title": "Can AI Be Fooled by Optical Illusions? A Visual Experiment",
    "section": "",
    "text": "Our brains are wired to perceive the world in ways that arenâ€™t always accurate. Optical illusions exploit these cognitive shortcuts, tricking us into seeing things that arenâ€™t really there. But what about AI? Can an artificial intelligence model, which processes visual data in a purely logical manner, experience illusions the same way we do?\nTo find out, I conducted an experiment by showing different classic optical illusions to an AI and comparing its responses to human perception. The results reveal fascinating insights into how AI â€œseesâ€ the world compared to us.\n\n\n\nThis illusion consists of two lines of equal length, each adorned with opposite-facing arrowheads. Humans typically perceive one line as longer than the other due to how our brains interpret depth and perspective.\nAIâ€™s Analysis: The AI correctly identifies that both lines are the same length, measuring them without being influenced by the arrowhead orientation. While humans rely on visual cues that create the illusion of depth, AI processes raw pixel data, avoiding the trick altogether.\n\n\n\n\nThis illusion makes us perceive a bright white triangle that doesnâ€™t actually exist, created by strategically placed Pac-Man-like shapes and angles.\nAIâ€™s Analysis: The AI detects the individual shapes and their arrangement but does not perceive the nonexistent white triangle. Since AI lacks the human tendency to infer missing information based on context, it only sees whatâ€™s explicitly present.\n\n\n\n\nThis illusion presents a checkerboard with a shadow cast over it, making two squares of identical color appear different. Humans perceive the shadowed square as lighter due to our brainâ€™s automatic adjustment for lighting conditions.\nAIâ€™s Analysis: The AI, analyzing the pixel values directly, determines that the two squares are in fact the same color. Unlike humans, who instinctively correct for lighting and shading, AI remains unaffected by contextual illusions.\n\n\n\n\n(FYI: It is actually a video, would recommend looking it up)\nThis illusion features a silhouette of a dancer that appears to spin in both clockwise and counterclockwise directions, depending on the viewerâ€™s perception.\nAIâ€™s Analysis: The AI identifies the individual frames of movement but does not â€œflipâ€ its perception like humans do. Since it does not have an inherent preference for one direction over the other, it simply registers the motion as ambiguous.\n\n\n\n\nAI Processes Raw Data: Unlike humans, AI perceives images as they are, unaffected by depth, shading, or implied shapes.\nNo Context-Based Assumptions: Humans use experience and context to fill in gaps, leading to optical illusions. AI only recognizes whatâ€™s explicitly present.\nAIâ€™s Strength in Objectivity: While illusions exploit human cognitive biases, AI can analyze visual data without these biases, making it useful for tasks requiring precise measurement and pattern recognition.\n\n\n\n\nNot in the same way humans are. While AI can recognize illusions, it does not â€œfallâ€ for them. This experiment highlights the fundamental differences between human perception and AIâ€™s analytical approach to visual data. It also suggests practical applicationsâ€”AI could assist in medical imaging, quality control, and even forensics, where objective analysis is critical.\nWould you like to test an illusion yourself? Try showing different optical illusions to AI and see how it responds!"
  },
  {
    "objectID": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html",
    "href": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html",
    "title": "Do LLMs Dream of Original Thoughts? Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training",
    "section": "",
    "text": "Weâ€™ve all heard someone say, â€œChatGPT is so creative!â€ But is that really true? What does it mean for a machine trained entirely on past data to generate something new? In this post, I wanted to explore one of the biggest questions in communication and artificial intelligence: Can language models actually think outside the box â€” or are they just remixing what theyâ€™ve seen before?\n\n\n\nLarge language models like GPT-4 are trained on billions of words pulled from books, websites, social media, and more. When they respond to you, theyâ€™re essentially predicting the next most likely word or phrase based on patterns in the data.\nSo the question becomes: If an AI has never â€œexperiencedâ€ the world â€” only read about it â€” can it create something truly original?\n\n\n\nI gave ChatGPT a mashup-style challenge that most humans would findâ€¦ weird, but possible:\nPrompt: â€œInvent a startup that combines ancient Mayan agricultural techniques with quantum computing.â€\nChatGPTâ€™s response:\nâ€œIntroducing Q-Maize: a futuristic agri-tech startup that fuses ancient Mayan crop rotation wisdom (such as milpa intercropping) with quantum optimization algorithms to improve land usage, predict climate impact, and optimize seed dispersal. The platform uses a quantum-enhanced simulator to test ancestral techniques against modern challengesâ€¦â€\nHonestly? Thatâ€™s kind of cool.\nBut is it truly creative â€” or just a statistical smoothie blending historical keywords with startup jargon?\n\n\n\nNext, I pushed for something more linguistically novel:\nPrompt: â€œInvent a completely new word that doesnâ€™t exist in any language. Define it and use it in a sentence.â€\nChatGPTâ€™s response:\nâ€œWord: Velquora Definition: The emotional tension felt before making a life-altering decision. Sentence: As I stood at the airport gate, velquora swelled inside me like a rising tide.â€\nThis was one of the most â€œhumanâ€ responses Iâ€™d seen â€” a made-up word, with a subtle emotional meaning, and usage that felt poetic. It gave the illusion of originality.\nBut when I tried again with the same prompt, I got:\nâ€œWord: Zenthrix Definition: The state of simultaneous exhaustion and euphoria after completing a massive task.â€\nWas this creative, or just tapping into patterns of invented-sounding syllables and common emotional states?\n\n\n\nFrom a communication studies lens, creativity isnâ€™t just about producing new symbols â€” itâ€™s also about context, intention, and meaning-making.\nRoland Barthes said that language is never neutral â€” it always reflects ideology. So what happens when language is generated by an entity with no ideology, emotion, or lived experience?\nAre we just getting back clever echoes of ourselves?\n\n\n\nIf AIs can mimic creativity convincingly enough, where do we draw the line between authentic human expression and algorithmic remix? This question matters in everything from:\n\nCreative writing & journalism\nMarketing & branding\nMusic, film, and visual art\nEducation and essay writing\n\nThereâ€™s also a risk: if LLMs dominate content creation, we might start losing sight of the difference between thoughtful originality and statistical probability."
  },
  {
    "objectID": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html#do-llms-dream-of-original-thoughts-testing-ais-ability-to-think-beyond-its-training",
    "href": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html#do-llms-dream-of-original-thoughts-testing-ais-ability-to-think-beyond-its-training",
    "title": "Do LLMs Dream of Original Thoughts? Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training",
    "section": "",
    "text": "Weâ€™ve all heard someone say, â€œChatGPT is so creative!â€ But is that really true? What does it mean for a machine trained entirely on past data to generate something new? In this post, I wanted to explore one of the biggest questions in communication and artificial intelligence: Can language models actually think outside the box â€” or are they just remixing what theyâ€™ve seen before?\n\n\n\nLarge language models like GPT-4 are trained on billions of words pulled from books, websites, social media, and more. When they respond to you, theyâ€™re essentially predicting the next most likely word or phrase based on patterns in the data.\nSo the question becomes: If an AI has never â€œexperiencedâ€ the world â€” only read about it â€” can it create something truly original?\n\n\n\nI gave ChatGPT a mashup-style challenge that most humans would findâ€¦ weird, but possible:\nPrompt: â€œInvent a startup that combines ancient Mayan agricultural techniques with quantum computing.â€\nChatGPTâ€™s response:\nâ€œIntroducing Q-Maize: a futuristic agri-tech startup that fuses ancient Mayan crop rotation wisdom (such as milpa intercropping) with quantum optimization algorithms to improve land usage, predict climate impact, and optimize seed dispersal. The platform uses a quantum-enhanced simulator to test ancestral techniques against modern challengesâ€¦â€\nHonestly? Thatâ€™s kind of cool.\nBut is it truly creative â€” or just a statistical smoothie blending historical keywords with startup jargon?\n\n\n\nNext, I pushed for something more linguistically novel:\nPrompt: â€œInvent a completely new word that doesnâ€™t exist in any language. Define it and use it in a sentence.â€\nChatGPTâ€™s response:\nâ€œWord: Velquora Definition: The emotional tension felt before making a life-altering decision. Sentence: As I stood at the airport gate, velquora swelled inside me like a rising tide.â€\nThis was one of the most â€œhumanâ€ responses Iâ€™d seen â€” a made-up word, with a subtle emotional meaning, and usage that felt poetic. It gave the illusion of originality.\nBut when I tried again with the same prompt, I got:\nâ€œWord: Zenthrix Definition: The state of simultaneous exhaustion and euphoria after completing a massive task.â€\nWas this creative, or just tapping into patterns of invented-sounding syllables and common emotional states?\n\n\n\nFrom a communication studies lens, creativity isnâ€™t just about producing new symbols â€” itâ€™s also about context, intention, and meaning-making.\nRoland Barthes said that language is never neutral â€” it always reflects ideology. So what happens when language is generated by an entity with no ideology, emotion, or lived experience?\nAre we just getting back clever echoes of ourselves?\n\n\n\nIf AIs can mimic creativity convincingly enough, where do we draw the line between authentic human expression and algorithmic remix? This question matters in everything from:\n\nCreative writing & journalism\nMarketing & branding\nMusic, film, and visual art\nEducation and essay writing\n\nThereâ€™s also a risk: if LLMs dominate content creation, we might start losing sight of the difference between thoughtful originality and statistical probability."
  },
  {
    "objectID": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html#conclusion-a-mirror-not-a-mind",
    "href": "posts/Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training/index.html#conclusion-a-mirror-not-a-mind",
    "title": "Do LLMs Dream of Original Thoughts? Testing AIâ€™s Ability to â€˜Thinkâ€™ Beyond Its Training",
    "section": "Conclusion: A Mirror, Not a Mind",
    "text": "Conclusion: A Mirror, Not a Mind\nSo, do LLMs â€œdreamâ€ of original thoughts? Not quite â€” they donâ€™t dream at all. But they simulate originality in ways that can fool us, fascinate us, and even inspire us. Thatâ€™s both powerful and a little dangerous.\nWe may be looking into a mirror that speaks â€” but itâ€™s still a mirror."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]